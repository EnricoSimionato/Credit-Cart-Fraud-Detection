{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To avoid problems you should execute the code by using Runtime -> Run all. It takes approximately 10-12 minutes to run all the sections in Colab."
      ],
      "metadata": {
        "id": "NePszjN-a3p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and libraries import\n",
        "\n",
        "Import of some libraries we use and loading of the dataset."
      ],
      "metadata": {
        "id": "QgPAoxqUClXN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udowETk_skjj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from IPython.display import display\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set size of the plots\n",
        "plt.rcParams[\"figure.figsize\"] = (15,10)"
      ],
      "metadata": {
        "id": "vF-Kbm98H8nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dPE2EZW7QkB"
      },
      "source": [
        "Link to the [dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset from dropbox\n",
        "!wget https://www.dropbox.com/s/ok3kty4rcyxyk9d/creditcard.csv\n",
        "\n",
        "creditcard_path = './creditcard.csv'\n",
        "\n",
        "# Import data from csv to a pandas dataframe\n",
        "data = pd.read_csv(creditcard_path)"
      ],
      "metadata": {
        "id": "cEmYYUKiUtw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTxDeFQ3zxAb"
      },
      "source": [
        "# Data inspection\n",
        "\n",
        "Display some basic information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-KPQ-i3tL1s"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy1e56i66hWo"
      },
      "outputs": [],
      "source": [
        "n_samples = data.shape[0]\n",
        "n_features = data.shape[1] - 1 # 'Class' is not a feature \n",
        "n_frauds = np.sum(data['Class'])\n",
        "\n",
        "print('%d samples' %n_samples)\n",
        "print('%d features' %n_features)\n",
        "print('%d frauds' %n_frauds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FI-ZRGZzXZo"
      },
      "outputs": [],
      "source": [
        "data.info() # Print datatypes, there aren't null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV4hRT51zZ_4"
      },
      "outputs": [],
      "source": [
        "data.describe() # Basic statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jio8rtx05YaI"
      },
      "source": [
        "# Attribute vectors\n",
        "\n",
        "Definition of the list of features of each dataset and of the datasets themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpRuQjoR09rY"
      },
      "outputs": [],
      "source": [
        "# Make the attribute vectors with the GA selected features\n",
        "v1 = ['V1', 'V5', 'V7', 'V8', 'V11', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'Amount', 'Class'] # 'Class' added\n",
        "data_1 = data[v1]\n",
        "v2 = ['V1', 'V6', 'V13', 'V16', 'V17', 'V22', 'V23', 'V28', 'Amount', 'Class']\n",
        "data_2 = data[v2]\n",
        "v3 = ['V2', 'V11', 'V12', 'V13', 'V15', 'V16', 'V17', 'V18', 'V20', 'V21', 'V24', 'V26', 'Amount', 'Class']\n",
        "data_3 = data[v3]\n",
        "v4 = ['V2', 'V7', 'V10', 'V13', 'V15', 'V17', 'V19', 'V28', 'Amount', 'Class']\n",
        "data_4 = data[v4]\n",
        "v5 = ['Time', 'V1', 'V7', 'V8', 'V9', 'V11', 'V12', 'V14', 'V15', 'V22', 'V27', 'V28', 'Amount', 'Class']\n",
        "data_5 = data[v5]\n",
        "# Dataset with full feature vector\n",
        "v6 = data.columns\n",
        "data_6 = data.copy()\n",
        "# Dataset with random feature vector\n",
        "v_random = ['V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V11', 'V12', 'V13', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V25', 'V26', 'V28', 'Amount', 'Class']\n",
        "data_7 = data[v_random]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E92bLkY4z9P1"
      },
      "outputs": [],
      "source": [
        "data_2.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiUgDiXE0CU3"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(22,10))\n",
        "sns.heatmap(data_2.corr(), annot = True, cmap = 'vlag_r', vmin = -1, vmax = 1,  ax = ax) \n",
        "# PCA was performed on V1, ..., V28 in fact all the covariances are very near to 0.\n",
        "# Only Time and Amount were not trasformed through PCA and so they have some covariance value\n",
        "# different from 0. It is possible to see that the Class, that represents the feature we want to\n",
        "# predict has negative coorelation with some features like V16 and V17."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mld65YMdAmQO"
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "Definition of the min-max scaling method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL8PfA9yApo5"
      },
      "outputs": [],
      "source": [
        "def min_max(data, data_min=None, data_max=None):\n",
        "  '''\n",
        "  Applies the min-max scaling method to a numpy dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data: ndarray\n",
        "    dataset to scale with samples on rows\n",
        "  data_min: ndarray, optional\n",
        "    array containing the min value for each feature\n",
        "  data_max: ndarray, optional\n",
        "    array containing the max value for each feature\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  data_normalized: ndarray\n",
        "    original dataset but normalized\n",
        "  data_min: ndarray\n",
        "    array containing the min value for each feature\n",
        "  data_max: ndarray\n",
        "    array containing the max value for each feature\n",
        "  '''\n",
        "\n",
        "  if data_min is None or data_max is None:\n",
        "    data_min = data.min(axis=0)\n",
        "    data_max = data.max(axis=0)\n",
        "\n",
        "  data_normalized = (data - data_min[None,:]) / (data_max[None,:] - data_min[None,:]) \n",
        "  return data_normalized, data_min, data_max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFOVCtun53bg"
      },
      "source": [
        "# Train-validation split\n",
        "\n",
        "Definition of the function splitting data into training, validation and test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uyw3FgyO54Lo"
      },
      "outputs": [],
      "source": [
        "def train_split(data_input, percentage_train=0.7, percentage_validation=0.0):\n",
        "  '''\n",
        "  Shuffles the dataset and splits in train, validation and test\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data_input: ndarray\n",
        "    dataset to split with samples on rows\n",
        "  percentage_train: float, optional\n",
        "    percentage of data to put in the train dataset\n",
        "  percentage_validation: float, optional\n",
        "    percentage of data to put in the validation dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x_train: ndarray\n",
        "    train dataset with samples\n",
        "  y_train: ndarray\n",
        "    train dataset with labels\n",
        "  x_valid: ndarray\n",
        "    validation dataset with samples\n",
        "  y_valid: ndarray\n",
        "    validation dataset with labels\n",
        "  x_test: ndarray\n",
        "    test dataset with samples\n",
        "  y_test: ndarray\n",
        "    test dataset with labels\n",
        "\n",
        "  Raises\n",
        "  ------\n",
        "  Exception: if the sum of the two percentages is greater than 1 or at least \n",
        "              one of them is negative\n",
        "  '''\n",
        "\n",
        "  # Check validity of the parameters\n",
        "  if percentage_train + percentage_validation > 1 or percentage_train < 0 or\\\n",
        "    percentage_validation < 0:\n",
        "    raise Exception('Percentages must be positive and their sum must be lower \\\n",
        "                    or equal to 1')\n",
        "    \n",
        "  data = data_input.copy()\n",
        "\n",
        "  np.random.seed(0) # For reproducibility\n",
        "  np.random.shuffle(data) # Shuffle along first axis, so rows\n",
        "\n",
        "  # Take the number of samples to put in each dataset\n",
        "  num_train = int(data.shape[0] * percentage_train) \n",
        "  num_val = num_train + int(data.shape[0] * percentage_validation) \n",
        "\n",
        "  # Split the dataset\n",
        "  x_train = data[:num_train, :-1] # Don't take last column that is the class\n",
        "  y_train = data[:num_train, -1:]\n",
        "  x_valid = data[num_train:num_val, :-1]\n",
        "  y_valid = data[num_train:num_val, -1:]\n",
        "  x_test =  data[num_val:, :-1] \n",
        "  y_test =  data[num_val:, -1:]\n",
        "\n",
        "  return x_train, y_train, x_valid, y_valid, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Undersampling\n",
        "\n",
        "Definition of the function doing undersampling on data."
      ],
      "metadata": {
        "id": "eq2mUxrLEAy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersample the majority class, this will create a balanced dataset, \n",
        "# allowing the classifier to better distinguish between the two classes.\n",
        "\n",
        "def undersample(x, y, ratio):\n",
        "  '''\n",
        "  Performs undersampling on the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    dataset to undersample with samples on \n",
        "  y: ndarray\n",
        "    corresponding labels of the dataset to undersample\n",
        "  ratio: float\n",
        "    desired ratio of samples between majority and minority class.\n",
        "    It assumes that label '1' is the minority class\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  x[idxs, :]: ndarray\n",
        "    dataset undersampled with samples\n",
        "  y[idxs, :]: ndarray\n",
        "    corresponding labels of the undersampled dataset\n",
        "  '''\n",
        "\n",
        "  # Take indexes corresponding to labels '0' and '1'\n",
        "  idxs_zeros = np.where(y == 0.)[0]\n",
        "  idxs_ones = np.where(y == 1.)[0]\n",
        "\n",
        "  # Compute the number of samples of the majority class to keep\n",
        "  n_samples = int(len(idxs_ones) * ratio)\n",
        "\n",
        "  # Extract randomly the samples from the majority class\n",
        "  idxs_zeros = np.random.choice(idxs_zeros, min(n_samples, len(idxs_zeros)), replace = False)\n",
        "  \n",
        "  # Extract the undersampled dataset\n",
        "  idxs = np.concatenate((idxs_zeros, idxs_ones))\n",
        "  return x[idxs, :], y[idxs, :]"
      ],
      "metadata": {
        "id": "ImeZXMgfECgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRvGJ2sVigOT"
      },
      "source": [
        "# Metrics\n",
        "\n",
        "Definition of the function computing the metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
        "\n",
        "def metrics(predictions, true_labels, metrics_df=None, dataset_label='NotLabeled'):\n",
        "  \"\"\"\n",
        "  Computes and print metrics TP, TN, FP, FN, AC, RC, PC, F1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  predictions: ndarray\n",
        "    predictions of samples obtained with a model\n",
        "  true_labels: ndarray\n",
        "    true labels of the samples\n",
        "  metrics_df: DataFrame, optional\n",
        "    DataFrame to which the computed statistics have to be put\n",
        "  dataset_label: str, optional\n",
        "    label identifing the belonging of the statistcs to its dataset\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  DataFrame\n",
        "    DataFrame containing the statistics contained in the parameter metrics_df\n",
        "    plus the statistics computed on the new predictions\n",
        "  \"\"\"\n",
        "\n",
        "  TP = np.sum(np.logical_and(predictions == 1., true_labels == 1.)) # Attacks accurately flagged as attacks\n",
        "  TN = np.sum(np.logical_and(predictions == 0., true_labels == 0.)) # Normal traffic accurately flagged as normal\n",
        "  FP = np.sum(np.logical_and(predictions == 1., true_labels == 0.)) # Normal traffic incorrectly flagged as attacks\n",
        "  FN = np.sum(np.logical_and(predictions == 0., true_labels == 1.)) # Attacks incorrectly flagged as normal \n",
        "\n",
        "  AC = ((TN + TP) / len(predictions)) * 100 # Accuracy\n",
        "  RC = (TP / (FN + TP)) * 100 # Recall or sensitivity\n",
        "  PR = (TP / (FP + TP)) * 100 # Precision\n",
        "  F1 = 2 * PR * RC / (PR + RC) # F1-Score\n",
        "\n",
        "  if metrics_df is None:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns)\n",
        "  else:\n",
        "    columns = ['Set of features', 'TP', 'TN', 'FP', 'FN', 'accuracy', 'recall', 'precision', 'F1-score']\n",
        "    metrics_df = metrics_df.append(pd.DataFrame([[dataset_label, TP, TN, FP, FN, AC, RC, PR, F1]], columns=columns))\n",
        "  \n",
        "  return metrics_df"
      ],
      "metadata": {
        "id": "2LMipb1OHRj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To save the metrics obatined on the test set by all classifiers\n",
        "metrics_per_classifier = list()\n",
        "\n",
        "# To plot ROC curve of all classifiers for each dataset\n",
        "ROC_curves_per_classifier = list()"
      ],
      "metadata": {
        "id": "tA8fh2aJBf5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9DA4mud_MUw"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions definition"
      ],
      "metadata": {
        "id": "VVszuLMFvBul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KyXk2vEHYIA"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x, weights, bias):\n",
        "  '''\n",
        "  Predicts output values using logistic regression\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    samples\n",
        "  weights: ndarray\n",
        "    weights of the logistic regression model\n",
        "  bias: float\n",
        "    bias of the logistic regression model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  predictions: ndarray\n",
        "    array with values of the samples predicted with the given model\n",
        "  '''\n",
        "  return 1. / (1. + jnp.exp(-(bias + x @ weights)))\n",
        "  \n",
        "def cross_entropy(x, y, weights, bias):\n",
        "  '''\n",
        "  Cross entropy cost function\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    samples\n",
        "  y: ndarray\n",
        "    true labels associated to samples\n",
        "  weights: ndarray\n",
        "    weights of the model\n",
        "  bias: float\n",
        "    bias of the model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    the value of the cost function with the given input\n",
        "  '''\n",
        "\n",
        "  # Compute prediction\n",
        "  y_pred = sigmoid(x, weights, bias)\n",
        "  # Compute loss\n",
        "  # penalize more error on frauds not detected, so the model pays more attention to this minority class\n",
        "  return - jnp.mean(2. * y * jnp.log(y_pred) + 0.5 * (1-y) * jnp.log(1-y_pred)) \n",
        "\n",
        "# Gradient computation and jit compilation of the cost function\n",
        "grad = jax.grad(cross_entropy, argnums = [2,3])\n",
        "grad_jit = jax.jit(grad)\n",
        "cross_entropy_jit = jax.jit(cross_entropy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A08AjL0L_LzX"
      },
      "outputs": [],
      "source": [
        "def SGD(x_train, y_train, x_valid=None, y_valid = None, num_epochs=25000, learning_rate_max=1e-1, \\\n",
        "        learning_rate_min = 1e-3, learning_rate_decay = 25000, batch_size = 4):\n",
        "  '''\n",
        "  SGD method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    train dataset with samples on rows\n",
        "  y_train: ndarray\n",
        "    train labels\n",
        "  x_valid: ndarray, optional\n",
        "    validation dataset with samples on rows\n",
        "  y_valid: ndarray, optional\n",
        "    validation labels\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform (default is 50000)\n",
        "  learning_rate_max: float, optional\n",
        "    max value of the learning rate (default is 1e-1)\n",
        "  learning_rate_min: float, optional\n",
        "    min value of the learning rate (default is 1e-3)\n",
        "  learning_rate_decay: int, optional\n",
        "    number of epochs to reach the learning_rate_min (default is 50000)\n",
        "  batch_size: int, optional\n",
        "    size of the batch (default is 32)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  weights: ndarray\n",
        "    computed weights of the model\n",
        "  bias: float\n",
        "    computed bias of the model\n",
        "  history: list\n",
        "    list with the history of the cost function values on training dataset\n",
        "  '''\n",
        "\n",
        "  # Check if there is a validation dataset\n",
        "  validation = True if x_valid is not None and y_valid is not None else False\n",
        "\n",
        "  # Initialize weights and bias\n",
        "  weights = np.random.randn(x_train.shape[1], 1) \n",
        "  bias = 0.0\n",
        "\n",
        "  # Initialize history of the cost function\n",
        "  history = list()\n",
        "  history.append(cross_entropy_jit(x_train, y_train, weights, bias))\n",
        "  if validation:\n",
        "    history_valid = list()\n",
        "    history_valid.append(cross_entropy_jit(x_valid, y_valid, weights, bias))\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):  \n",
        "    # Take indexes of the batch\n",
        "    idxs = np.random.choice(x_train.shape[0], batch_size) \n",
        "\n",
        "    # Compute gradient\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], weights, bias)\n",
        "    \n",
        "    # Compute learning rate value\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "    \n",
        "    # Update weights and bias\n",
        "    weights -= learning_rate * grads[0]\n",
        "    bias -= learning_rate * grads[1]\n",
        "\n",
        "    # Compute value of the cost function on entire dataset, it is done periodically because\n",
        "    # if the dataset is big, this operation slows down too much the function\n",
        "    if epoch % 10 == 0:\n",
        "      history.append(cross_entropy_jit(x_train, y_train, weights, bias)) \n",
        "      if validation:\n",
        "        history_valid.append(cross_entropy_jit(x_valid, y_valid, weights, bias))\n",
        "\n",
        "  return weights, bias, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REyZTIE9KquG"
      },
      "outputs": [],
      "source": [
        "def predict_LR(x, weights, bias):\n",
        "  '''\n",
        "  Predicts labels using logistic regression\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    samples\n",
        "  weights: ndarray\n",
        "    weights of the logistic regression model\n",
        "  bias: float\n",
        "    bias of the logistic regression model\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  predictions: ndarray\n",
        "    array with labels of the samples predicted with the given model\n",
        "  '''\n",
        "\n",
        "  # Predict output of samples with the model\n",
        "  y_pred = sigmoid(x, weights, bias)\n",
        "\n",
        "  # Link the predicted output to the label\n",
        "  predictions = np.array([0. if pred < 0.5 else 1. for pred in y_pred])[:, None] \n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLyQMU-KzPAZ"
      },
      "source": [
        "## Testing LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbujhvbzSPj"
      },
      "outputs": [],
      "source": [
        "# Test LR on a single dataset\n",
        "\n",
        "# Set up the dataset\n",
        "data_np = data_5.to_numpy()\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "x_train, y_train = undersample(x_train, y_train, 50.)\n",
        "\n",
        "# Train model\n",
        "weights, bias, history = SGD(x_train, y_train) \n",
        "\n",
        "# Plot history of the values of loss function\n",
        "plt.plot(history)\n",
        "plt.title('train loss: %1.3e' % history[-1])\n",
        "\n",
        "# Get predictions with the trained model\n",
        "predictions = predict_LR(x_train, weights, bias)\n",
        "\n",
        "# Compute metrics on train test\n",
        "metrics_train = metrics(predictions, y_train, dataset_label='v5'+' training')\n",
        "  \n",
        "# Normalizing the test set\n",
        "x_test, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Get predictions with the trained model\n",
        "predictions = predict_LR(x_test, weights, bias) \n",
        "\n",
        "# Compute metrics\n",
        "metrics(predictions, y_test, metrics_df=metrics_train, dataset_label='v5'+' test').style.hide_index()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing LR on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "fig, axs = plt.subplots(8,1,figsize = (15,80)) \n",
        "metrics_train_set = None\n",
        "metrics_test_set = None\n",
        "weights_list = list()\n",
        "bias_list = list()\n",
        "ROC_curves = dict()\n",
        "\n",
        "# Train the classifiers\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "\n",
        "  # Splitting train set and test set\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  \n",
        "  # Normalizing the training set\n",
        "  x_train, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  \n",
        "  # Undersampling the training set\n",
        "  x_train, y_train = undersample(x_train, y_train, 20.)\n",
        "\n",
        "  # Train model\n",
        "  # using default values so num_epochs=25000, learning_rate_max=1e-1, \n",
        "  # learning_rate_min = 1e-3, learning_rate_decay = 25000, batch_size = 4\n",
        "  weights, bias, history = SGD(x_train, y_train) \n",
        "\n",
        "  # Save the model\n",
        "  weights_list.append(weights)\n",
        "  bias_list.append(bias)\n",
        "  \n",
        "  # Plot training history of the values of loss function\n",
        "  axs[i].plot(history)\n",
        "  axs[i].set_title('train loss for dataset %d: %1.3e' % (i+1, history[-1]))\n",
        "\n",
        "  # Get predictions with the trained model on train set\n",
        "  predictions = predict_LR(x_train, weights, bias) \n",
        "  \n",
        "  # Compute metrics on train test\n",
        "  metrics_train_set = metrics(predictions, y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "   \n",
        "  # Normalizing the test set\n",
        "  x_test, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Get predictions with the trained model on test set\n",
        "  predictions = predict_LR(x_test, weights, bias) \n",
        "\n",
        "  # Compute metrics on test set\n",
        "  metrics_test_set = metrics(predictions, y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "\n",
        "  # Compute ROC and AUC\n",
        "  y_pred = sigmoid(x_test, weights, bias)\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  axs[-1].plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "  # Save the ROC curve\n",
        "  ROC_curves[i] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "\n",
        "# Save all the ROC curves for this classifier to plot them in the end\n",
        "ROC_curves_per_classifier.append(ROC_curves)\n",
        "\n",
        "axs[-1].set_title('ROC Curve')\n",
        "axs[-1].legend(loc='lower right')\n",
        "axs[-1].set_xlabel('False Positive Rate')\n",
        "axs[-1].set_ylabel('True Positive Rate')"
      ],
      "metadata": {
        "id": "lmz_RM5-XZK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the train set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "NEYCN8KKH7n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics\n",
        "metrics_per_classifier.append(metrics_test_set)\n",
        "\n",
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "cH9alkHhH8xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree"
      ],
      "metadata": {
        "id": "wqDFCfjD7jDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree model definition"
      ],
      "metadata": {
        "id": "BUHTi4jUvLNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node: \n",
        "  '''\n",
        "  Class representing a node in a DecisionTree\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  feature: int\n",
        "    feature associated to the node\n",
        "  threshold: float\n",
        "    threshold associated to the feature\n",
        "  left: Node\n",
        "    left child of the node\n",
        "  right: Node\n",
        "    right child of the node\n",
        "  value: float\n",
        "    label associated to the node, if leaf\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  is_leaf: tell if a Node is a leaf or not\n",
        "  '''\n",
        " \n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    feature: int, optional\n",
        "      feature associated to the node (default is None)\n",
        "    threshold: float, optional\n",
        "      threshold associated to the feature (default is None)\n",
        "    left: Node, optional\n",
        "      left child of the node (default is None)\n",
        "    right: Node, optional\n",
        "      right child of the node (default is None)\n",
        "    value: float, optional\n",
        "      label associated to the node, if leaf (default is None)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Node\n",
        "      node created\n",
        "    '''\n",
        "\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "\n",
        "  def is_leaf(self):\n",
        "    '''\n",
        "    Tells if the node is a leaf\n",
        "    '''\n",
        "\n",
        "    return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "  '''\n",
        "  Class representing a decision tree\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  max_depth: int\n",
        "    max depth of a tree\n",
        "  n_features: int\n",
        "    number of features to evaluate while branching to build a tree\n",
        "  max_number_thresholds: int\n",
        "    max number of thresholds to evaluate while branching to build a tree\n",
        "  min_samples: int\n",
        "    min number of samples in a node to branch when building a tree\n",
        "  root: Node\n",
        "    root of the tree\n",
        "  mode: str\n",
        "      mode to compute the information gain, it can be 'gini' or 'entropy'\n",
        "  \n",
        "  Methods\n",
        "  -------\n",
        "  train: to train a decision tree model\n",
        "  predict: to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, max_depth=10, n_features=None, max_number_thresholds=np.inf, min_samples=2, mode='gini'):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_depth: int, optional\n",
        "      max depth of a tree (default is 10)\n",
        "    n_features: int, optional\n",
        "      number of features to evaluate while branching to build a tree (default is None)\n",
        "    max_number_thresholds: int, optional\n",
        "      max number of thresholds to evaluate while branching to build a tree (default is np.inf)\n",
        "    min_samples: int, optional\n",
        "      min number of samples in a node to branch when building a tree (default is 2)\n",
        "    mode: str, optional\n",
        "      mode to compute the information gain, it can be 'gini' or 'entropy' (default is 'gini')\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    DecisionTree\n",
        "      decision tree model created\n",
        "    \n",
        "    '''\n",
        "\n",
        "    self.max_depth = max_depth \n",
        "    self.n_features = n_features \n",
        "    self.max_number_thresholds = max_number_thresholds \n",
        "    self.min_samples = min_samples  \n",
        "    self.root = None\n",
        "    self.mode = mode\n",
        "\n",
        "  def train(self, x, y):\n",
        "    '''\n",
        "    Builds the tree\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      train dataset with samples on rows \n",
        "    y: ndarray\n",
        "      labels associated to the dataset\n",
        "    '''\n",
        "\n",
        "    if self.n_features is None or self.n_features > x.shape[1]:\n",
        "      self.n_features = x.shape[1] \n",
        "    self.root = self.build_tree(x, y)\n",
        "      \n",
        "  def build_tree(self, x, y, depth=0):\n",
        "    '''\n",
        "    Builds the tree recursively and returns the root\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      train dataset with samples on rows\n",
        "    y: ndarray\n",
        "      labels associated to the dataset\n",
        "    depth: int, optional\n",
        "      current depth of the tree\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Node\n",
        "      the root of the built tree\n",
        "    '''\n",
        "\n",
        "    num_samples, num_features = x.shape\n",
        "    num_values = len(np.unique(y)) # Get number of different values in y \n",
        "\n",
        "    # Check the stopping criteria, if so create a leaf node\n",
        "    if depth >= self.max_depth or num_samples < self.min_samples or num_values <= 1:\n",
        "      return Node(value = self.get_label(y))\n",
        "\n",
        "    # Randomly select the features to evaluate\n",
        "    features = np.random.choice(num_features, self.n_features, replace=False)\n",
        "\n",
        "    # To select the best branch\n",
        "    max_gain = -1\n",
        "    branch_threshold = None\n",
        "    branch_feature = None\n",
        "\n",
        "    # Evaluate all selected features\n",
        "    for feature in features:\n",
        "      # Randomly select the thresholds to evaluate\n",
        "      thresholds = np.random.choice(np.unique(x[:, feature]), \n",
        "                                    min(self.max_number_thresholds, len(np.unique(x[:, feature]))), replace=False)\n",
        "\n",
        "      # Evaluate all selected thresholds\n",
        "      for threshold in thresholds:\n",
        "        # Compute the information gain\n",
        "        gain = self.information_gain(x[:, feature], y, threshold)\n",
        "\n",
        "        # Check if it is the best gain\n",
        "        if gain > max_gain:\n",
        "          max_gain = gain\n",
        "          branch_threshold = threshold\n",
        "          branch_feature = feature\n",
        "\n",
        "    # Build left branch\n",
        "    left_idxs = x[:, branch_feature] <= branch_threshold\n",
        "    left = self.build_tree(x[left_idxs, :], y[left_idxs, :], depth+1)\n",
        "\n",
        "    # Build right branch\n",
        "    right_idxs = x[:, branch_feature] > branch_threshold\n",
        "    right = self.build_tree(x[right_idxs, :], y[right_idxs, :], depth+1)\n",
        "\n",
        "    return Node(branch_feature, branch_threshold, left, right)\n",
        "        \n",
        "  def information_gain(self, x, y, threshold):\n",
        "    '''\n",
        "    Computes the information gain using gini or entropy\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      train dataset with samples on rows\n",
        "    y: ndarray\n",
        "      labels associated to the dataset\n",
        "    threshold: float\n",
        "      threshold to divide the samples\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the information gain \n",
        "    '''\n",
        "\n",
        "    # Take indexes of left and right children\n",
        "    left_idxs = x <= threshold\n",
        "    right_idxs = x > threshold\n",
        "\n",
        "    # If one of the children is empty nothing is changing, so the gain is 0\n",
        "    if np.sum(left_idxs) == 0 or np.sum(right_idxs) == 0:\n",
        "      return 0.\n",
        "\n",
        "    # Compute the weights of the children\n",
        "    weight_left = np.sum(left_idxs) / len(y)\n",
        "    weight_right = np.sum(right_idxs) / len(y)\n",
        "\n",
        "    # Compute information gain based on the mode\n",
        "    if self.mode == 'gini':\n",
        "      return self.gini(y) - (weight_left * self.gini(y[left_idxs, :]) + weight_right * self.gini(y[right_idxs, :]))\n",
        "    elif self.mode == 'entropy':\n",
        "      return self.entropy(y) - (weight_left * self.entropy(y[left_idxs, :]) + weight_right * self.entropy(y[right_idxs, :]))\n",
        "\n",
        "  def gini(self, y):\n",
        "    '''\n",
        "    Computes the gini index of a vector of labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: ndarray\n",
        "      vector of labels containing values 0. or 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the gini index\n",
        "    '''\n",
        "\n",
        "    # Compute probability of having 1. and 0.\n",
        "    p_ones = np.sum(y) / len(y)\n",
        "    p_zeros = 1. - p_ones\n",
        "\n",
        "    # Compute gini index\n",
        "    return 1. - (p_zeros**2 + p_ones**2)\n",
        "\n",
        "  # Using gini would not require the logarithm, more efficient\n",
        "  def entropy(self, y): \n",
        "    '''\n",
        "    Computes the entropy of a vector of labels\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y: ndarray\n",
        "      vector of labels containing values 0. or 1. \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the entropy of the vector\n",
        "    '''\n",
        "\n",
        "    # Compute probability of having 1. and 0.\n",
        "    p_ones = np.sum(y) / len(y)\n",
        "    p_zeros = 1. - p_ones\n",
        "\n",
        "    # Compute entropy\n",
        "    # To avoid computing logarithm of 0.\n",
        "    if p_ones == 0. or  p_zeros == 0.:\n",
        "      return 0\n",
        "    return -(p_zeros * np.log2(p_zeros) + p_ones * np.log2(p_ones))\n",
        "\n",
        "  def get_label(self, y):\n",
        "    '''\n",
        "    Finds the most common label in a vector\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    : ndarray\n",
        "      vector of labels containing values 0. or 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      the value of the most common label\n",
        "    '''\n",
        "\n",
        "    ones = np.sum(y)\n",
        "    zeros = len(y) - ones\n",
        "    return 1. if ones >= zeros else 0.\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    Returns the predictions on the input dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      test dataset with samples on rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      predicted labels\n",
        "    '''\n",
        "    \n",
        "    # Compute prediction for each sample of the input\n",
        "    return np.array([self.get_prediction(sample, self.root) for sample in x])[:,None] # Return a column vector\n",
        "\n",
        "  def get_prediction(self, x, node):\n",
        "    '''\n",
        "    Recursively traverse the tree to make a prediction\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      sample to predict\n",
        "    node: Node\n",
        "      current node of the tree\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      predicted label\n",
        "    '''\n",
        "\n",
        "    # If the node is a leaf return the prediction\n",
        "    if node.is_leaf():\n",
        "      return node.value\n",
        "    \n",
        "    # If the value of the sample corresponding to the feature of the node \n",
        "    # is less than the threshold of the node go left, otherwise go right\n",
        "    if x[node.feature] <= node.threshold:\n",
        "      return self.get_prediction(x, node.left)\n",
        "    else:\n",
        "      return self.get_prediction(x, node.right)"
      ],
      "metadata": {
        "id": "Unqbtpmi7m4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing DT"
      ],
      "metadata": {
        "id": "hkXbr1i1snCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test DT on a single dataset\n",
        "\n",
        "# Set up the dataset\n",
        "data_np = data_5.to_numpy()\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "\n",
        "# Train model\n",
        "# higher max_threshold or n_features performs better but takes more time to train\n",
        "classifier = DecisionTree(max_depth=7, n_features=3, max_number_thresholds=20, min_samples=5)\n",
        "classifier.train(x_train, y_train)\n",
        "\n",
        "# Get predictions with the trained model\n",
        "predictions = classifier.predict(x_train)\n",
        "\n",
        "# Compute metrics on train test\n",
        "metrics_train = metrics(predictions, y_train, dataset_label='v5'+' training')\n",
        "\n",
        "# Normalizing the test set\n",
        "x_test, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Get predictions with the trained model\n",
        "predictions = classifier.predict(x_test)\n",
        "\n",
        "# Compute metrics\n",
        "metrics(predictions, y_test, metrics_df=metrics_train, dataset_label='v5'+' test').style.hide_index() "
      ],
      "metadata": {
        "id": "a_DLCjlL76JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing DT on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "metrics_train_set = None\n",
        "metrics_test_set = None\n",
        "classifiers = list()\n",
        "ROC_curves = dict()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "\n",
        "  # Splitting train set and test set\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  \n",
        "  # Normalizing the training set\n",
        "  x_train, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  \n",
        "  # Undersampling the training set\n",
        "  x_train, y_train = undersample(x_train, y_train, 200.)\n",
        "\n",
        "  # Train model\n",
        "  classifier = DecisionTree(max_depth=7, n_features=int(x_train.shape[1]/2), max_number_thresholds=50) \n",
        "  classifier.train(x_train, y_train)\n",
        "\n",
        "  # Save the model\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # Get predictions with the trained model on train set\n",
        "  predictions = classifier.predict(x_train)\n",
        "\n",
        "  # Compute metrics on train set\n",
        "  metrics_train_set = metrics(predictions, y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Normalizing the test set\n",
        "  x_test, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Get predictions with the trained model on test set\n",
        "  predictions = classifier.predict(x_test)\n",
        "\n",
        "  # Compute metrics on test set\n",
        "  metrics_test_set = metrics(predictions, y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "\n",
        "  # Compute ROC and AUC\n",
        "  fpr, tpr, _ = roc_curve(y_test, predictions)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "\n",
        "  # Save the ROC curve\n",
        "  ROC_curves[i] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "\n",
        "# Save all the ROC curves for this classifier to plot them in the end\n",
        "ROC_curves_per_classifier.append(ROC_curves)\n",
        "\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NHcIJqEfdm9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the train set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "ozIDs4iurLzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics\n",
        "metrics_per_classifier.append(metrics_test_set)\n",
        "\n",
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "HJ2jYAZprMpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest"
      ],
      "metadata": {
        "id": "dHzCLi29W6Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest model definition"
      ],
      "metadata": {
        "id": "9I6qNEjkvXQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "  '''\n",
        "  Class representing a random forest\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  n_trees: int \n",
        "    number of trees in the forest\n",
        "  max_depth: int \n",
        "    max depth of a tree\n",
        "  n_features: int\n",
        "    number of features to evaluate while branching to build a tree\n",
        "  max_number_thresholds: int\n",
        "    max number of thresholds to evaluate while branching to build a tree\n",
        "  min_samples: int\n",
        "    min number of samples in a node to branch when building a tree\n",
        "  trees: list\n",
        "    list with all the trees\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  train: to train a random forest model\n",
        "  predict: to make predictions using the built model\n",
        "  '''\n",
        "\n",
        "  def __init__(self, n_trees=20, max_depth=10, n_features=None, max_number_thresholds=np.inf, min_samples=2):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_trees: int, optional\n",
        "      number of trees in the forest (default is 50)\n",
        "    max_depth: int, optional\n",
        "      max depth of a tree (default is 10)\n",
        "    n_features: int, optional\n",
        "      number of features to evaluate while branching to build a tree (default is None)\n",
        "    max_number_thresholds: int, optional\n",
        "      max number of thresholds to evaluate while branching to build a tree (default is np.inf)\n",
        "    min_samples: int, optional\n",
        "      min number of samples in a node to branch when building a tree (default is 2)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    RandomForest\n",
        "      random forest model created\n",
        "    '''\n",
        "\n",
        "    self.n_trees = n_trees\n",
        "    self.max_depth = max_depth \n",
        "    self.n_features = n_features \n",
        "    self.max_number_thresholds = max_number_thresholds \n",
        "    self.min_samples = min_samples\n",
        "    self.trees = []\n",
        "  \n",
        "  def train(self, x, y):\n",
        "    '''\n",
        "    Builds the forest\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      train dataset with samples on rows \n",
        "    y: ndarray\n",
        "      labels associated to the dataset\n",
        "    ''' \n",
        "\n",
        "    for i in tqdm(range(self.n_trees)):\n",
        "      # Create bootstrapped dataset\n",
        "      n_samples = x.shape[0]\n",
        "      idxs = np.random.choice(n_samples, n_samples, replace=True) # With replacement\n",
        "      \n",
        "      # Train a tree\n",
        "      tree = DecisionTree(self.max_depth, self.n_features, self.max_number_thresholds, self.min_samples)\n",
        "      tree.train(x[idxs, :], y[idxs, :])\n",
        "      self.trees.append(tree)\n",
        "\n",
        "  def predict(self, x):\n",
        "    '''\n",
        "    Returns the predictions by majority vote\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      test dataset with samples on rows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    majorities: ndarray\n",
        "      predicted labels\n",
        "    '''\n",
        "\n",
        "    # Make predictions for each tree \n",
        "    votes = np.array([tree.predict(x) for tree in self.trees])\n",
        "    \n",
        "    # Compute majority \n",
        "    majorities = np.mean(votes, axis=0)\n",
        "    \n",
        "    # Associate the labels\n",
        "    majorities[majorities >= 0.5] = 1.\n",
        "    majorities[majorities < 0.5] = 0.\n",
        "    return majorities, votes"
      ],
      "metadata": {
        "id": "Xz467gI3W-GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing RF"
      ],
      "metadata": {
        "id": "5zW--vvdXnUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test RF on a single dataset\n",
        "\n",
        "# Set up the dataset\n",
        "data_np = data_5.to_numpy()\n",
        "x_train, y_train, _, _, x_test, y_test = train_split(data_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "x_train, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "x_train, y_train = undersample(x_train, y_train, 50.)\n",
        "\n",
        "# Train model\n",
        "classifier = RandomForest(n_trees=20, max_depth=5, n_features=3, max_number_thresholds=50) \n",
        "classifier.train(x_train, y_train)\n",
        "\n",
        "# Get predictions with the trained model\n",
        "predictions,_ = classifier.predict(x_train)\n",
        "\n",
        "# Compute metrics on train test\n",
        "metrics_train = metrics(predictions, y_train, dataset_label='v5'+' training')\n",
        "\n",
        "# Normalizing the test set\n",
        "x_test, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Get predictions with the trained model\n",
        "predictions,_ = classifier.predict(x_test)\n",
        "\n",
        "# Compute metrics\n",
        "metrics(predictions, y_test, metrics_df=metrics_train, dataset_label='v5'+' test').style.hide_index() "
      ],
      "metadata": {
        "id": "ibr4snEMXOCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing RF on all datasets \n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "metrics_train_set = None\n",
        "metrics_test_set = None\n",
        "classifiers = list()\n",
        "ROC_curves = dict()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Pre-processing the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "\n",
        "  # Splitting train set and test set\n",
        "  x_train, y_train, _, _, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.0)\n",
        "  \n",
        "  # Normalizing the training set\n",
        "  x_train, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  \n",
        "  # Undersampling the training set\n",
        "  x_train, y_train = undersample(x_train, y_train, 200.)\n",
        "\n",
        "  # Train model\n",
        "  # Slightly better with 20 trees, but the training time doubles\n",
        "  classifier = RandomForest(n_trees=10, max_depth=7, n_features=3, max_number_thresholds=50)  # better with 20 trees instead of 50\n",
        "  classifier.train(x_train, y_train)\n",
        "\n",
        "  # Save the model\n",
        "  classifiers.append(classifier)\n",
        "\n",
        "  # Get predictions with the trained model on train set\n",
        "  predictions, _ = classifier.predict(x_train)\n",
        "\n",
        "  # Compute metrics on train test\n",
        "  metrics_train_set = metrics(predictions, y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Normalizing the test set\n",
        "  x_test, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Get predictions with the trained model\n",
        "  predictions, votes = classifier.predict(x_test)\n",
        "\n",
        "  # Compute metrics on test set\n",
        "  metrics_test_set = metrics(predictions, y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "   \n",
        "  # Compute majority \n",
        "  majorities = np.mean(votes, axis=0)\n",
        "\n",
        "  # Compute ROC and AUC\n",
        "  fpr, tpr, _ = roc_curve(y_test, majorities)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "\n",
        "  # Save the ROC curve\n",
        "  ROC_curves[i] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "\n",
        "# Save all the ROC curves for this classifier to plot them in the end\n",
        "ROC_curves_per_classifier.append(ROC_curves)\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TWl-JuujYGC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the train set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "4T7yJpR4t8gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics\n",
        "metrics_per_classifier.append(metrics_test_set)\n",
        "\n",
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "pZupR5H8t6Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes\n",
        "\n"
      ],
      "metadata": {
        "id": "V4vMDXN5UNHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian naive Bayes model definition"
      ],
      "metadata": {
        "id": "GvBk76kTi1Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of a class representing the gaussian naive Bayes model\n",
        "class GaussianBayesModel:\n",
        "  '''\n",
        "  Gaussian naive Bayes model, binary classification\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  probability_true: float\n",
        "    probability of a sample to belong to the positive class\n",
        "  mean_features_given_true: ndarray\n",
        "    means of the features considering the samples belonging to the positive class\n",
        "  mean_features_given_false: ndarray\n",
        "    means of the features considering the samples belonging to the negative class\n",
        "  stddev_features_given_true: ndarray\n",
        "    standard deviation of the features considering the samples belonging to the positive class\n",
        "  stddev_features_given_false: ndarray\n",
        "    standard deviation of the features considering the samples belonging to the negative class\n",
        "  name_features: list\n",
        "    names of the features used in the model\n",
        "  \n",
        "  Methods\n",
        "  -------\n",
        "  plot_distributions:\n",
        "    plots the distributions of the features of the dataset\n",
        "  predict:\n",
        "    to make predictions using the built model\n",
        "  get_probability_positive:\n",
        "    returns the \"probability\" of some samples to be in the positive class\n",
        "  '''\n",
        "\n",
        "  def __init__(self, x, y, name_features):\n",
        "    '''\n",
        "    Initializes the gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset used for computing the model\n",
        "    y: ndarray\n",
        "      labels for each sample of the dataset representing the class of the sample\n",
        "    name_features: list\n",
        "      names of the features of the dataset\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    GaussianBayesModel\n",
        "      computed gaussian naive Bayes model\n",
        "    '''\n",
        "    \n",
        "    # Computing the samples belonging to the positive class\n",
        "    x_true = x[(y.reshape(y.shape[0])==1),:]\n",
        "    # Computing the samples belonging to the negative class\n",
        "    x_false = x[(y.reshape(y.shape[0])==0),:]\n",
        "    \n",
        "    # Computing the probability of a sample to belong to the positive class\n",
        "    self.probability_true = x_true.shape[0]/x.shape[0]\n",
        "\n",
        "    # Computing means and standard deviations of the features of the samples\n",
        "    # belonging to the positive class\n",
        "    self.mean_features_given_true = np.mean(x_true, axis=0)[None,:]\n",
        "    self.stddev_features_given_true = np.std(x_true, axis=0, ddof=1)[None,:]\n",
        "\n",
        "    # Computing means and standard deviations of the features of the samples\n",
        "    # belonging to the negative class\n",
        "    self.mean_features_given_false = np.mean(x_false, axis=0)[None,:]\n",
        "    self.stddev_features_given_false = np.std(x_false, axis=0, ddof=1)[None,:]\n",
        "    \n",
        "    # Storing the name of the features of the dataset\n",
        "    self.name_features = name_features\n",
        "\n",
        "  def __str__(self):\n",
        "    '''\n",
        "    Returns a textual representation of the gaussian naive Bayes model\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "      string representing the model\n",
        "    '''\n",
        "\n",
        "    # Defining the string representing the model as concatenation of the attributes\n",
        "    # mean_features_given_true, mean_features_given_false, stddev_features_given_true\n",
        "    # and stddev_features_given_false\n",
        "    model_string = 'Means of the features given the positiveness:\\n'+ \\\n",
        "      str(self.mean_features_given_true) + \\\n",
        "      '\\nMeans of the features given the negativeness:\\n' + \\\n",
        "      str(self.mean_features_given_false) + \\\n",
        "      '\\n Standard deviations of the features given the positiveness:\\n' + \\\n",
        "      str(self.stddev_features_given_true) + \\\n",
        "      '\\n Standard deviations of the features given the negativeness:\\n '+ \\\n",
        "      str(self.stddev_features_given_true)\n",
        "    \n",
        "    return model_string\n",
        "\n",
        "  def plot_distributions(self, label_positive='true', label_negative='false', sample=None):\n",
        "    '''\n",
        "    Plots the gaussian distributions representing the distribution of the \n",
        "    features given the classes: p(feature|label = 1), p(feature|label = 0)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    label_positive: str, optional\n",
        "      label of the class which is the searched positive one\n",
        "    label_negative: str, optional\n",
        "      label of the other class which is the negative one\n",
        "    sample: ndarray, optional\n",
        "      if not None, its features will be displyed on the plots\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the classes of the given samples\n",
        "    '''\n",
        "\n",
        "    fig,axs = plt.subplots(self.mean_features_given_true.shape[1], 1, \n",
        "                           figsize=(6, self.mean_features_given_true.shape[1]*6))\n",
        "    axs=axs.flatten()\n",
        "\n",
        "    for i in range(self.mean_features_given_true.shape[1]):\n",
        "      # Taking the x values between [mean - 4 * std, mean + 4 * std] in order to\n",
        "      # plot a significant part of the bell of the gaussian distribution\n",
        "      x = np.linspace(self.mean_features_given_true[0,i] - 4 * self.stddev_features_given_true[0,i], \n",
        "          self.mean_features_given_true[0,i] + 4 * self.stddev_features_given_true[0,i], 200)\n",
        "      \n",
        "      # Plotting the gaussian distributions corresponding to the distribution of\n",
        "      # the features for the sample belonging to the positive class (fraud)\n",
        "      axs[i].plot(x, (np.exp(-(x - self.mean_features_given_true[0,i]) ** 2\n",
        "                             / (2 * self.stddev_features_given_true[0,i] ** 2))\n",
        "                      / (np.sqrt(2 * np.pi) * self.stddev_features_given_true[0,i])),\n",
        "                  label=(str(i) + ' ' if self.name_features is None \\\n",
        "                         else self.name_features[i] + ' ') + label_positive)\n",
        "      \n",
        "      # Taking the x values between [mean - 4 * std, mean + 4 * std] in order to\n",
        "      # plot a significant part of the bell of the gaussian distribution\n",
        "      x = np.linspace(self.mean_features_given_false[0,i] - 4 * self.stddev_features_given_false[0,i],\n",
        "          self.mean_features_given_false[0,i] + 4 * self.stddev_features_given_false[0,i], 200)\n",
        "      \n",
        "      # Plotting the gaussian distributions corresponding to the distribution of\n",
        "      # the features for the sample belonging to the negative class (not fraud)\n",
        "      axs[i].plot(x, (np.exp(-(x - self.mean_features_given_false[0,i]) ** 2\n",
        "                             / (2 * self.stddev_features_given_false[0,i] ** 2))\n",
        "                      / (np.sqrt(2 * np.pi) * self.stddev_features_given_false[0,i])), \n",
        "                  label=(str(i) + ' ' if self.name_features is None \\\n",
        "                         else self.name_features[i] + ' ') + label_negative)\n",
        "      \n",
        "      # Plotting a line corresponding to the values of the given sample\n",
        "      if sample is not None:\n",
        "        axs[i].axvline(x=sample[i], color='red')\n",
        "      axs[i].legend()\n",
        "\n",
        "  def predict(self, x, safe_prediction_parameter=1.):\n",
        "    '''\n",
        "    Predicts the classes to which belongs the given samples basing on the\n",
        "    gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset composed by the samples of which the labels have to be predicted\n",
        "    safe_prediction_parameter: float, optional\n",
        "      parameter for changing the relationship between the likelihood of the\n",
        "      positiveness and the negativeness needed for stating that a sample is\n",
        "      positive. Starting with value 1 and increasing it, it will be needed a\n",
        "      lower confidence for classifing a transaction as fraud \n",
        "\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the classes of the given samples\n",
        "    '''\n",
        "\n",
        "    data = x\n",
        "\n",
        "    # Computing probability of the given samples to belong to the positive class\n",
        "    # given their features (applying the log to all terms)\n",
        "    probability_true_given_features = np.sum(-((data - self.mean_features_given_true) ** 2)\n",
        "                                              / (2 * self.stddev_features_given_true ** 2)\n",
        "                                             - np.log(np.sqrt(2 * np.pi) * self.stddev_features_given_true), axis=1)\n",
        "    probability_true_given_features += np.log(self.probability_true)\n",
        "\n",
        "    # Computing probability of the given samples to belong to the negative class\n",
        "    # given their features (applying the log to all terms)\n",
        "    probability_false_given_features = np.sum(-((data - self.mean_features_given_false) ** 2)\n",
        "                                               / (2*self.stddev_features_given_false ** 2)\n",
        "                                              - np.log(np.sqrt(2 * np.pi) * self.stddev_features_given_false), axis=1)\n",
        "    probability_false_given_features += np.log(1 - self.probability_true)\n",
        "    \n",
        "    # Computing the prediction considering the one with the class with the gratest\n",
        "    # log-probability\n",
        "    y_pred = np.array([1 if probability_true_given_features[i] >= \\\n",
        "                      safe_prediction_parameter * probability_false_given_features[i] else 0 for i in range(x.shape[0])])\n",
        "\n",
        "    return y_pred\n",
        "  \n",
        "  def get_probability_positive(self, x):\n",
        "    '''\n",
        "    Returns the probability of some samples to be in the positive class basing\n",
        "    on the gaussian naive Bayes model\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray\n",
        "      dataset composed by the samples of which the labels have to be predicted\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray\n",
        "      ordered labels representing the probability of the samples to belong to\n",
        "      the positive class\n",
        "    '''\n",
        "\n",
        "    data = x\n",
        "\n",
        "    # Computing probability of the given samples to belong to the positive class\n",
        "    # given their features (applying the log to all terms)\n",
        "    probability_true_given_features = np.sum(np.exp(-((data - self.mean_features_given_true) ** 2)\n",
        "                                                    / (2 * self.stddev_features_given_true ** 2)) \n",
        "                                                    / np.log(np.sqrt(2 * np.pi) * self.stddev_features_given_true), axis=1)\n",
        "    probability_true_given_features *= np.log(self.probability_true)\n",
        "\n",
        "    # Computing probability of the given samples to belong to the negative class\n",
        "    # given their features (applying the log to all terms)\n",
        "    probability_false_given_features = np.sum(np.exp(-((data - self.mean_features_given_false) ** 2) \n",
        "                                                    / (2 * self.stddev_features_given_false ** 2)) \n",
        "                                                    / np.log(np.sqrt(2 * np.pi) * self.stddev_features_given_false), axis=1)\n",
        "    probability_false_given_features *= np.log(1 - self.probability_true)\n",
        "    \n",
        "    # Computing the probability of the samples to belong to the positive class\n",
        "    y_pred = probability_true_given_features / (probability_true_given_features + probability_false_given_features)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "3yzISV-2k5H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting of the models and testing NB"
      ],
      "metadata": {
        "id": "CQ1lJI6UUNsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 7 sets of selected features\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_train_set = None\n",
        "metrics_valid_set = None\n",
        "metrics_test_set = None\n",
        "gnb_models = list()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Creating the numpy array of the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting train set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the train set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "\n",
        "  # Defining the model\n",
        "  gm = GaussianBayesModel(x_train_normalized, y_train, features_names[i])\n",
        "  gnb_models.append(gm)\n",
        "\n",
        "  # Normalizing the validation set\n",
        "  x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "  # Normalizing the test set\n",
        "  x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Getting predictions with the trained model on train set\n",
        "  y_predicted = gm.predict(x_train_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_train_set = metrics(y_predicted[:,None], y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Getting predictions with the trained model on validation set\n",
        "  y_predicted = gm.predict(x_valid_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_valid_set = metrics(y_predicted[:,None], y_valid, metrics_df=metrics_valid_set, dataset_label='v'+str(i+1)+' validation')\n",
        "  \n",
        "  # Getting predictions with the trained model on test set\n",
        "  y_predicted = gm.predict(x_test_normalized, safe_prediction_parameter=1)\n",
        "\n",
        "  # Computing metrics\n",
        "  metrics_test_set = metrics(y_predicted[:,None], y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "  \n",
        "  yday = gm.get_probability_positive(x_test_normalized)\n",
        "\n",
        "  # Compute ROC and AUC\n",
        "  fpr, tpr, _ = roc_curve(y_test, yday)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  \n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "thrxzl2k568B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "dSgA3puC_omB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style.hide_index()"
      ],
      "metadata": {
        "id": "gaYZ1gR6_pp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "COnl2CYw_puK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 7 sets of selected features with\n",
        "# undersampling of the dataset\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_train_set = None\n",
        "metrics_valid_set = None\n",
        "metrics_test_set = None\n",
        "gnb_models_undersample = list()\n",
        "ROC_curves = dict()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Creating the numpy array of the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting train set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the training set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  # Undersampling the training set\n",
        "  x_train_normalized, y_train = undersample(x_train_normalized, y_train, 50.)\n",
        "\n",
        "  # Defining the model\n",
        "  gm = GaussianBayesModel(x_train_normalized, y_train, features_names[i])\n",
        "  gnb_models_undersample.append(gm)\n",
        "\n",
        "  # Normalizing the validation set\n",
        "  x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "  # Normalizing the test set\n",
        "  x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Getting predictions with the trained model on train set\n",
        "  y_predicted = gm.predict(x_train_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_train_set = metrics(y_predicted[:,None], y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Getting predictions with the trained model on validation set\n",
        "  y_predicted = gm.predict(x_valid_normalized, safe_prediction_parameter=1)\n",
        "  # Computing metrics\n",
        "  metrics_valid_set = metrics(y_predicted[:,None], y_valid, metrics_df=metrics_valid_set, dataset_label='v'+str(i+1)+' validation')\n",
        "  \n",
        "  # Getting predictions with the trained model on test set\n",
        "  y_predicted = gm.predict(x_test_normalized, safe_prediction_parameter=1)\n",
        "\n",
        "  # Computing metrics\n",
        "  metrics_test_set = metrics(y_predicted[:,None], y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "\n",
        "  probabilities = gm.get_probability_positive(x_test_normalized)\n",
        "\n",
        "  # Compute ROC and AUC\n",
        "  fpr, tpr, _ = roc_curve(y_test, probabilities)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  \n",
        "  # Plot the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "\n",
        "  # Save the ROC curve\n",
        "  ROC_curves[i] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "\n",
        "# Save all the ROC curves for this classifier to plot them in the end\n",
        "ROC_curves_per_classifier.append(ROC_curves)\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2y1fJL5tCX2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "drJyq5TdAWTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style.hide_index()"
      ],
      "metadata": {
        "id": "5l32nZ6CAWV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics\n",
        "metrics_per_classifier.append(metrics_test_set)\n",
        "\n",
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "QUDpmiWPAT5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods for inspecting the model"
      ],
      "metadata": {
        "id": "8VDBlDnIY1-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and testing of the model without undersampling\n",
        "\n",
        "# Splitting train set, validation set and test set\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(data_1.to_numpy(), percentage_train=0.7, percentage_validation=0.1)\n",
        "# Normalizing the training set\n",
        "x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "\n",
        "# Defining the model\n",
        "gm = GaussianBayesModel(x_train_normalized, y_train, v1)\n",
        "\n",
        "# Normalizing the test set\n",
        "x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Getting predictions with the trained model\n",
        "y_predicted = gm.predict(x_test_normalized, safe_prediction_parameter=1)\n",
        "\n",
        "# Computing metrics\n",
        "metrics(y_predicted[:,None], y_test, dataset_label='v1'+' test').style.hide_index()"
      ],
      "metadata": {
        "id": "XIDFoOv1Z_Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and testing of the model with undersampling\n",
        "\n",
        "# Splitting train set, validation set and test set\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(data_1.to_numpy(), percentage_train=0.7, percentage_validation=0.1)\n",
        "# Normalizing the training set\n",
        "x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "# Undersampling the training set\n",
        "x_train_normalized, y_train = undersample(x_train_normalized, y_train, 80.)\n",
        "\n",
        "# Defining the model\n",
        "gm_undersampling = GaussianBayesModel(x_train_normalized, y_train, v1)\n",
        "\n",
        "# Normalizing the test set\n",
        "x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Getting predictions with the trained model\n",
        "y_predicted = gm_undersampling.predict(x_test_normalized, safe_prediction_parameter=1)\n",
        "\n",
        "# Computing metrics\n",
        "metrics(y_predicted[:,None], y_test, dataset_label='v1'+' test').style.hide_index()"
      ],
      "metadata": {
        "id": "A0Hi-U1TUTgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the gaussian distribution representing the features\n",
        "gm.plot_distributions(label_positive='fraud', label_negative='not fraud', sample=x_test_normalized[0,:])\n",
        "print('The transaction is not fraud') if gm.predict(x_test_normalized[0,:][None,:])[0] == 0 else print('The transaction is a fraud')"
      ],
      "metadata": {
        "id": "YWwl1eTUhmBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional demonstration about the distribution of the features"
      ],
      "metadata": {
        "id": "Gy7T_mjACGeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The approximation of the distribution of the features to a gaussian distribution\n",
        "# is supported by the following plot in which is possible to see that the\n",
        "# distribution of the values of the features are approximately normal\n",
        "\n",
        "# Plotting the distribution of the values of a feature of the training dataset\n",
        "sns.histplot(x_train[:,10], kde = True)\n",
        "# Analogous for the other features"
      ],
      "metadata": {
        "id": "KXUKXZMRKMYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial neural network"
      ],
      "metadata": {
        "id": "rTQWdgH-7UVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define two possible ways to solve the problem using artificial neural network, which are very similar but with small differences.\n",
        "The first method uses an artificial neural network with one neuron in the output layer. If the value of the output will be closer to 1 than to 0 (greater or equal to 0.5) the transaction will be classified as fraud, otherwise the transaction will be classified as not fraud.\n",
        "The second method uses an artificial neural network with two neurons in the output layer. The outputs will represent the probability of the transaction given in input to be fraud or to be not a fraud.\n",
        "\n",
        "The function needed for running the neural network are the same or very similar in the two cases but they are written twice in order to make possible to separately run the two sections.\n",
        "\n",
        "Before running the training of the models and their testing phase, it is needed to run the \"Function definition\" subsection of the specific neural network that has to be created."
      ],
      "metadata": {
        "id": "1OYRFxd87oUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN with one neuron in the output layer"
      ],
      "metadata": {
        "id": "CQdEnB8xF98C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions definition"
      ],
      "metadata": {
        "id": "0WPzs8DxjOP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layers_size):\n",
        "  '''\n",
        "  Returns the parameters of the artificial neural network given the number of\n",
        "  neurons of its layers, namely it returns the matrix of weights and the bias\n",
        "  vector for each layer \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  layers_size: list\n",
        "    ordered sizes of the layers of the artificial neural network it is required\n",
        "    to be created\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  list\n",
        "    parameters of the artificial neural network\n",
        "  '''\n",
        "\n",
        "  np.random.seed(0) # For reproducibility\n",
        "  parameters = list()\n",
        "\n",
        "  for i in range(len(layers_size) - 1):\n",
        "    W = np.random.randn(layers_size[i+1], layers_size[i])\n",
        "    b = np.zeros((layers_size[i+1], 1))\n",
        "    parameters.append(W)\n",
        "    parameters.append(b)\n",
        "  \n",
        "  return parameters"
      ],
      "metadata": {
        "id": "0t7cHl3O7lG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN(x, parameters):\n",
        "  '''\n",
        "  Commputes the value of the output of the artificial neural network identified\n",
        "  by its parameters given an input\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    output value of the artificial neural network\n",
        "\n",
        "  Raises\n",
        "  ------\n",
        "  Exception\n",
        "    if the size of the list containing the parameters is odd, it must be even\n",
        "  '''\n",
        "\n",
        "  if len(parameters)%2!=0:\n",
        "    raise Exception(\"The input parameters must be in even number\")\n",
        "  \n",
        "  layer = x.T\n",
        "  \n",
        "  num_layers = int(len(parameters)/2)+1\n",
        "  weights = parameters[0::2]\n",
        "  biases = parameters[1::2]\n",
        "\n",
        "  for i in range(num_layers-1):\n",
        "    layer = weights[i] @ layer - biases[i]\n",
        "    \n",
        "    # Activation function is applied to all the layers\n",
        "    if i == num_layers-2:\n",
        "      # On the output layer it is applied the sigmoid since the output is \n",
        "      # needed to be between 0 and 1\n",
        "      layer = jax.nn.sigmoid(layer)\n",
        "    else:\n",
        "      # Only one of the following three lines has to be uncommented, the\n",
        "      # application of a different activation function ends up in a different set\n",
        "      # of hyperparameters to reach a good resulting model.\n",
        "      # The sigmoid function needs a bigger network than the other two for\n",
        "      # reaching a good result.\n",
        "\n",
        "      layer = jnp.tanh(layer)\n",
        "      #layer = jax.nn.relu(layer)\n",
        "      #layer = jax.nn.sigmoid(layer)\n",
        "\n",
        "  layer = layer.T\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "A8ufUeKQ7pm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(x, y, parameters, class_weights=np.array([1.,1.])):\n",
        "  '''\n",
        "  Computes the cross entropy cost function in the case of an artificial neural\n",
        "  network for a binary classification problem\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    cross entropy between the predictions of the artificial neural network\n",
        "    and the correct value\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return -jnp.mean(class_weights[0] * y * jnp.log(y_pred) + class_weights[1] * (1 - y) * jnp.log(1 - y_pred))\n",
        "\n",
        "def MSE(x, y, parameters):\n",
        "  '''\n",
        "  Computes the mean squared error in the case of an artificial neural network\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    mean square error between the predictions of the artificial neural network\n",
        "    and the correct value\n",
        "  '''\n",
        "\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return jnp.mean(jnp.square(y - y_pred))/2\n",
        "\n",
        "def accuracy(x, y, parameters, threshold=0.5):\n",
        "  '''\n",
        "  Compute the accuracy of the prediction in the case of an artificial neural\n",
        "  network for a binary classification problem\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  threshold: float, optional\n",
        "    output value of the artificial neural network over which a sample is\n",
        "    considered to belong to the positive class\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    accuracy of the artificial neural network, namely the number of samples\n",
        "    correctly classified divided by the total number of samples\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  labels_pred = y_pred >= threshold\n",
        "  return jnp.mean(y == labels_pred)\n",
        "\n",
        "# Computating the JIT version of the loss functions\n",
        "cross_entropy_jit = jax.jit(cross_entropy)\n",
        "MSE_jit = jax.jit(MSE)\n",
        "accuracy_jit = jax.jit(accuracy)"
      ],
      "metadata": {
        "id": "qzUCZimF7sSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSW(params):\n",
        "  '''\n",
        "  Computes the sum of the squared values of the weights of the artificial neural\n",
        "  network\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    sum of the squared values of the weights of the artificial neural network\n",
        "  '''\n",
        "  weights = params[::2] # extracting the weights\n",
        "  # Initializing the sum of the squared weights\n",
        "  partial_sum = 0.0\n",
        "  # Initializing the counter containing the number of parameters\n",
        "  n_weights = 0\n",
        "  for W in weights:\n",
        "    partial_sum += jnp.sum(W * W)\n",
        "    n_weights += W.shape[0] * W.shape[1]\n",
        "  return partial_sum / n_weights\n",
        "\n",
        "def cost_regularization(x, y, params, penalization, class_weights=np.array([1.,1.])):\n",
        "  '''\n",
        "  Computes the cost function applying regularization. It is used MSW for being\n",
        "  able to regularize and the cross entropy for binary classification problem as\n",
        "  standard cost function\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  penalization:\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    cross entropy between the predictions of the artificial neural network\n",
        "    and the correct values with regularization term\n",
        "  '''\n",
        "  return cross_entropy(x, y, params, class_weights=class_weights) + penalization * MSW(params)"
      ],
      "metadata": {
        "id": "3CU1wu7bI6wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(x_train, y_train, parameters, histories, x_valid=None, y_valid=None):\n",
        "  '''\n",
        "  Updates the history of the values of the loss functions for the training set\n",
        "  and for the validation set\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    input of the artificial neural network used for the training phase\n",
        "  y_train: ndarray\n",
        "    correct value of the output of the training set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  histories: dict\n",
        "    current history of the loss functions to which a new measurement has to be\n",
        "    added\n",
        "  x_valid: ndarray, optional\n",
        "    input of the artificial neural network used for validating the model\n",
        "  y_valid: ndarray, optional\n",
        "    correct value of the output of the validation set\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  dict\n",
        "    histories of the loss functions\n",
        "  '''\n",
        "  # The comments can be removed in order to return also the histories with\n",
        "  # respect to the other cost funcions and not only with respect to the cross\n",
        "  # entropy\n",
        "  if histories is None or len(histories) == 0:\n",
        "    histories = {'Xen_train': []}\n",
        "    histories['MSE_train'] = []\n",
        "    histories['acc_train'] = []\n",
        "  if x_valid is not None and len(x_valid) != 0 and y_valid is not None and len(y_valid) != 0 and 'Xen_valid' not in histories.keys():\n",
        "    histories['Xen_valid'] = []\n",
        "    histories['MSE_valid'] = []\n",
        "    histories['acc_valid'] = []\n",
        "  elif x_valid is not None and len(x_valid) != 0 and y_valid is not None and len(y_valid) != 0:\n",
        "    histories['Xen_valid'].append(cross_entropy_jit(x_valid, y_valid, parameters))\n",
        "    histories['MSE_valid'].append(MSE_jit(x_valid, y_valid, parameters))\n",
        "    histories['acc_valid'].append(accuracy_jit(x_valid, y_valid, parameters))\n",
        "  histories['Xen_train'].append(cross_entropy_jit(x_train, y_train, parameters))\n",
        "  histories['MSE_train'].append(MSE_jit(x_train, y_train, parameters))\n",
        "  histories['acc_train'].append(accuracy_jit(x_train, y_train, parameters))\n",
        "  return histories"
      ],
      "metadata": {
        "id": "WEF2x3bqEYgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(histories):\n",
        "  '''\n",
        "  Plots the histories of the losses during execution\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  histories: dict\n",
        "    values of the loss functions over time to be plotted\n",
        "  '''\n",
        "  \n",
        "  fig, axs = plt.subplots(len(histories), 1, figsize=(8, 8*len(histories)))\n",
        "  axs = axs.flatten()\n",
        "  for i, key in enumerate(histories):\n",
        "    axs[i].loglog(histories[key])\n",
        "    axs[i].set_title(list(histories.keys())[i])"
      ],
      "metadata": {
        "id": "uY-Nzy-5FMy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(x_train, y_train, x_valid, y_valid, parameters, num_epochs=1000, batch_size=128,\n",
        "        learning_rate_min=1e-3, learning_rate_max=1e-1, learning_rate_decay=1000,\n",
        "        penalization=1., class_weights=np.array([1.,1.])):\n",
        "  \"\"\"\n",
        "  Stochastic xgradient descent method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient\n",
        "  learning_rate_min: float, optional\n",
        "    minimum learning rate used in the training phase\n",
        "  learning_rate_max: float, optional\n",
        "    maximum learning rate used in the training phase\n",
        "  learning_rate_decay: float, optional\n",
        "    learning rate decay used in the training phase\n",
        "  penalization: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term, using penalization = 0, regularization is\n",
        "    not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "\n",
        "  num_samples = x_train.shape[0]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  # Gradient of the cost function composed by the cross entropy for binary\n",
        "  # classification problems and the regularization term\n",
        "  grad_jit = jax.jit(jax.grad(cost_regularization, argnums = 2))\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], parameters, penalization, class_weights=class_weights)\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      parameters[i] -= learning_rate * grads[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "RTLoCYThppg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(x_train, y_train, x_valid, y_valid, parameters, num_epochs=1000, batch_size=128,\n",
        "        learning_rate_min=1e-3, learning_rate_max=1e-1, learning_rate_decay=1000,\n",
        "        decay_rate=0.9, penalization=1., class_weights=np.array([1.,1.])):\n",
        "  \"\"\"\n",
        "  Implements the Nesterov accelleration method with mini-batch and learning rate\n",
        "  decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient\n",
        "  learning_rate_min: float, optional\n",
        "    minimum learning rate used in the training phase\n",
        "  learning_rate_max: float, optional\n",
        "    maximum learning rate used in the training phase\n",
        "  learning_rate_decay: float, optional\n",
        "    learning rate decay used in the training phase\n",
        "  decay_rate: float, optional\n",
        "    weight of the velocity vector at each iteration\n",
        "  penalization: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term, using penalization = 0, regularization is\n",
        "    not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "  \n",
        "  num_samples = x_train.shape[0]\n",
        "  velocity = [0.0 for i in range(len(parameters))]\n",
        "  grad_args = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  # Gradient of the cost function composed by the cross entropy for binary\n",
        "  # classification problems and the regularization term\n",
        "  grad_jit = jax.jit(jax.grad(cost_regularization, argnums = 2))\n",
        "  \n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      grad_args[i] = parameters[i] - decay_rate * velocity[i]\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], grad_args, penalization, class_weights=class_weights)\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      velocity[i] = decay_rate * velocity[i] + learning_rate * grads[i]\n",
        "      parameters[i] -= velocity[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "GRo18nNu-y63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSprop(x_train, y_train, x_valid, y_valid, parameters, num_epochs=1000,\n",
        "            batch_size=128, learning_rate=1e-3, decay_rate=0.8, penalization=1.,\n",
        "            class_weights=np.array([1.,1.])):\n",
        "  \"\"\"\n",
        "  Implements the RMSprop method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient\n",
        "  learning_rate: float, optional\n",
        "    learning rate used in the training phase\n",
        "  decay_rate: float, optional\n",
        "    weight used for weighting the cumulative square grad vector at each iteration\n",
        "  penalization: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term, using penalization = 0, regularization is\n",
        "    not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "  \n",
        "  delta = 1e-7\n",
        "  num_samples = x_train.shape[0]\n",
        "  cumulative_square_grad = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  # Gradient of the cost function composed by the cross entropy for binary\n",
        "  # classification problems and the regularization term\n",
        "  grad_jit = jax.jit(jax.grad(cost_regularization, argnums = 2))\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], parameters, penalization, class_weights=class_weights)\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      cumulative_square_grad[i] = decay_rate * cumulative_square_grad[i] + (1 - decay_rate) * grads[i] * grads[i]\n",
        "      parameters[i] -= learning_rate * grads[i] / (delta + jnp.sqrt(cumulative_square_grad[i]))\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "QjgjZvtcXrtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size,\n",
        "                          opt_method='RMSprop', num_epochs=1000, batch_size=128,\n",
        "                          learning_rate=1e-3, learning_rate_min=1e-3,\n",
        "                          learning_rate_max=1e-1, learning_rate_decay=1000,\n",
        "                          history_decay=0.9, regularization_parameter=1.,\n",
        "                          class_weights=np.array([1.,1.])):\n",
        "  '''\n",
        "  Trains an artificial neural network given the labeled data.\n",
        "\n",
        "  The classificator will be created using the given sizes and activation\n",
        "  function on all the layers, except for the last one. On the last layer is\n",
        "  applied the softmax activation function.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  layers_size: list \n",
        "    ordered sizes of the layers of the artificial neural network\n",
        "  opt_method: str, optional\n",
        "    chosen optimization method, the possible values are 'SGD', 'NAG' and\n",
        "    'RMSprop', stochastic gradient descent, Nesterov accelleration method and\n",
        "    RMSprop respectively\n",
        "  num_epochs: int, optional\n",
        "    number of epochs of the chosen optimization method to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient in the chosen\n",
        "    optimization method\n",
        "  learning_rate: float, optional\n",
        "    learning rate used in the training phase; here considered only for RMSprop\n",
        "  learning_rate_min: float, optional\n",
        "    minimum learning rate used in the training phase when using learning rate\n",
        "    decay; here considered only for SGD and NAG\n",
        "  learning_rate_max: float, optional\n",
        "    maximum learning rate used in the training phase when using learning rate\n",
        "    decay; here considered only for SGD and NAG\n",
        "  learning_rate_decay: float, optional\n",
        "    learning rate decay used in the training phase when using learning rate\n",
        "    decay; here considered only for SGD and NAG\n",
        "  history_decay: float, optional\n",
        "    weight used for weighting the history of gradients at each iteration when\n",
        "    the optimization method uses the history of gradients; here considered only\n",
        "    for NAG and RMSprop\n",
        "  regularization_parameter: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term in the chosen optimization method, using\n",
        "    penalization = 0, regularization is not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    parameters of the trained artificial neural network\n",
        "  history: dict\n",
        "    history of the loss functions during the training phase\n",
        "  '''\n",
        "  \n",
        "  parameters = initialize_parameters(layers_size)\n",
        "  if opt_method == 'SGD':\n",
        "    ANN_params, history = SGD(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                              num_epochs=num_epochs, batch_size=batch_size, learning_rate_min=learning_rate_min,\n",
        "                              learning_rate_max=learning_rate_max, learning_rate_decay=learning_rate_decay,\n",
        "                              penalization=regularization_parameter, class_weights=class_weights)\n",
        "  elif opt_method == 'NAG':\n",
        "    ANN_params, history = NAG(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                              num_epochs=num_epochs, batch_size=batch_size, learning_rate_min=learning_rate_min,\n",
        "                              learning_rate_max=learning_rate_max, learning_rate_decay=learning_rate_decay,\n",
        "                              decay_rate=history_decay, penalization=regularization_parameter,\n",
        "                              class_weights=class_weights)\n",
        "  elif opt_method == 'RMSprop':\n",
        "    ANN_params, history = RMSprop(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                                  num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                                  decay_rate=history_decay, penalization=regularization_parameter,\n",
        "                                  class_weights=class_weights)\n",
        "  else:\n",
        "    ANN_params, history = RMSprop(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                                  num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                                  decay_rate=history_decay, penalization=regularization_parameter,\n",
        "                                  class_weights=class_weights)\n",
        "\n",
        "  return ANN_params, history"
      ],
      "metadata": {
        "id": "3fMiuuGB7u3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing ANN\n",
        "\n",
        "Run the previous subsection before running the training and testing of the artificial neural network."
      ],
      "metadata": {
        "id": "ruJORmuj7xwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the random dataset with undersampling of the\n",
        "# dataset\n",
        "\n",
        "dataset_index = 7\n",
        "# Creating the numpy array of the dataset\n",
        "dataset_np = data_7.to_numpy()\n",
        "# Splitting training set, validation set and test set\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "# Normalizing the training set\n",
        "x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "# Undersampling the training set\n",
        "x_train_normalized, y_train = undersample(x_train_normalized, y_train, 50.)\n",
        "\n",
        "# Normalizing the validation set\n",
        "x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "# Normalizing the test set\n",
        "x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Definition of the model\n",
        "layers_size = [x_train.shape[1],30,20,1]\n",
        "\n",
        "# Training the ANN\n",
        "ANN_params, history = fit_classificator_ANN(x_train_normalized, y_train, x_valid_normalized, y_valid,\n",
        "                                            layers_size, opt_method='RMSprop', num_epochs=2000, batch_size=256,\n",
        "                                            learning_rate=1e-3, history_decay=0.8, regularization_parameter=0.5,\n",
        "                                            class_weights=np.array([2.8,1]))\n",
        "\n",
        "# Plotting the training history\n",
        "plot_history(history)\n",
        "\n",
        "# Getting predictions with the trained model on train set\n",
        "y_predicted = ANN(x_train_normalized, ANN_params)\n",
        "y_predicted = y_predicted >= 0.5\n",
        "\n",
        "# Computing metrics for the training set\n",
        "metrics_train_set = metrics(y_predicted, y_train, dataset_label='v'+str(dataset_index)+' training')\n",
        "\n",
        "# Getting predictions with the trained model on validation set\n",
        "y_predicted = ANN(x_valid_normalized, ANN_params)\n",
        "y_predicted = y_predicted >= 0.5\n",
        "\n",
        "# Computing metrics for the validation set\n",
        "metrics_valid_set = metrics(y_predicted, y_valid, dataset_label='v'+str(dataset_index)+' validation')\n",
        "\n",
        "# Getting predictions with the trained model on test set\n",
        "y_predicted = ANN(x_test_normalized, ANN_params) # different name for using the probabilities in the plot of the ROC curve\n",
        "y_predicted = y_predicted >= 0.5\n",
        "\n",
        "# Computing metrics for the test set\n",
        "metrics_test_set = metrics(y_predicted, y_test, dataset_label='v'+str(dataset_index)+' test')\n",
        "\n",
        "metrics_test_set"
      ],
      "metadata": {
        "id": "7ZR8_gDnT_cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "3sqdNXf8VU09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style.hide_index()"
      ],
      "metadata": {
        "id": "F24BTk4kVmKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "5KBG2OZDVmNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 7 sets of selected features with\n",
        "# undersampling of the dataset\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_train_set = None\n",
        "metrics_valid_set = None\n",
        "metrics_test_set = None\n",
        "ANN_parameters = list()\n",
        "histories = list()\n",
        "ROC_curves = dict()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Creating the numpy array of the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting training set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the training set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  # Undersampling the training set\n",
        "  x_train_normalized, y_train = undersample(x_train_normalized, y_train, 50.)\n",
        "\n",
        "  # Normalizing the validation set\n",
        "  x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "  # Normalizing the test set\n",
        "  x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Definition of the model\n",
        "  layers_size = [x_train.shape[1],30,20,1]\n",
        "\n",
        " # Training the ANN\n",
        "  ANN_params, history = fit_classificator_ANN(x_train_normalized, y_train, x_valid_normalized, y_valid,\n",
        "                                              layers_size, opt_method='RMSprop', num_epochs=2000, batch_size=256,\n",
        "                                              learning_rate=1e-3, history_decay=0.8, regularization_parameter=0.5,\n",
        "                                              class_weights=np.array([2.8,1]))\n",
        "  ANN_parameters.append(ANN_params)\n",
        "  # Plotting the training history\n",
        "  histories.append(history)\n",
        "\n",
        "  # Getting predictions with the trained model on train set\n",
        "  y_predicted = ANN(x_train_normalized, ANN_params)\n",
        "  y_predicted = y_predicted >= 0.5\n",
        "  # Computing metrics for the training set\n",
        "  metrics_train_set = metrics(y_predicted, y_train, metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Getting predictions with the trained model on validation set\n",
        "  y_predicted = ANN(x_valid_normalized, ANN_params)\n",
        "  y_predicted = y_predicted >= 0.5\n",
        "  # Computing metrics for the validation set\n",
        "  metrics_valid_set = metrics(y_predicted, y_valid, metrics_df=metrics_valid_set, dataset_label='v'+str(i+1)+' validation')\n",
        "  \n",
        "  # Getting predictions with the trained model on test set\n",
        "  y_probabilities = ANN(x_test_normalized, ANN_params) # different name for using the probabilities in the plot of the ROC curve\n",
        "  y_predicted = y_probabilities >= 0.5\n",
        "  # Computing metrics for the test set\n",
        "  metrics_test_set = metrics(y_predicted, y_test, metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "  \n",
        "  # Compute ROC and AUC\n",
        "  fpr, tpr, _ = roc_curve(y_test, y_probabilities)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  \n",
        "  # Plotting the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "\n",
        "  # Save the ROC curve\n",
        "  ROC_curves[i] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "\n",
        "# Save all the ROC curves for this classifier to plot them in the end\n",
        "ROC_curves_per_classifier.append(ROC_curves)\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vBAHHeRSjoGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "wBRSXUfMRS9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style.hide_index()"
      ],
      "metadata": {
        "id": "nc47CPwWRTBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics\n",
        "metrics_per_classifier.append(metrics_test_set)\n",
        "\n",
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "jyvnscz4RTFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN with two neurons in the output layer"
      ],
      "metadata": {
        "id": "CKai0lyJ4Uz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions definition"
      ],
      "metadata": {
        "id": "hoyqheJRGP0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layers_size):\n",
        "  '''\n",
        "  Returns the parameters of the artificial neural network given the number of\n",
        "  neurons of its layers, namely it returns the matrix of weights and the bias\n",
        "  vector for each layer \n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  layers_size: list\n",
        "    ordered sizes of the layers of the artificial neural network it is required\n",
        "    to be created\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  list\n",
        "    parameters of the artificial neural network\n",
        "  '''\n",
        "\n",
        "  np.random.seed(0) # For reproducibility\n",
        "  parameters = list()\n",
        "\n",
        "  for i in range(len(layers_size) - 1):\n",
        "    W = np.random.randn(layers_size[i+1], layers_size[i])\n",
        "    b = np.zeros((layers_size[i+1], 1))\n",
        "    parameters.append(W)\n",
        "    parameters.append(b)\n",
        "  \n",
        "  return parameters"
      ],
      "metadata": {
        "id": "ZvJWS9WfHM_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ANN(x, parameters):\n",
        "  '''\n",
        "  Commputes the value of the output of the artificial neural network identified\n",
        "  by its parameters given an input, considering a last layer which is the\n",
        "  softmax layer\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  ndarray\n",
        "    output value of the artificial neural network\n",
        "\n",
        "  Raises\n",
        "  ------\n",
        "  Exception\n",
        "    if the size of the list containing the parameters is odd, it must be even\n",
        "  '''\n",
        "\n",
        "  if len(parameters)%2!=0:\n",
        "    raise Exception(\"The input parameters must be in even number\")\n",
        "\n",
        "  layer = x.T  \n",
        "  \n",
        "  num_layers = int(len(parameters)/2)+1\n",
        "  weights = parameters[0::2]\n",
        "  biases = parameters[1::2]\n",
        "\n",
        "  for i in range(num_layers-1):\n",
        "    layer = weights[i] @ layer - biases[i]\n",
        "    \n",
        "    # Activation function is applied to all the layers\n",
        "\n",
        "    # Only one of the following three lines has to be uncommented, the\n",
        "    # application of a different activation function ends up in a different set\n",
        "    # of hyperparameters to reach a good resulting model.\n",
        "    # The sigmoid function needs a bigger network than the other two for\n",
        "    # reaching a good result.\n",
        "      \n",
        "    layer = jnp.tanh(layer)\n",
        "    #layer = jax.nn.relu(layer)\n",
        "    #layer = jax.nn.sigmoid(layer)\n",
        "  \n",
        "  # On the output layer it is applied a softmax function since the output is \n",
        "  # needed to be between 0 and 1 and to be a probability\n",
        "  den = jnp.sum(jnp.exp(layer), axis = 0)\n",
        "  layer = jnp.exp(layer) / den\n",
        "\n",
        "  layer = layer.T\n",
        "  \n",
        "  return layer"
      ],
      "metadata": {
        "id": "8qGNZoOm46LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_general(x, y, parameters, class_weights=np.array([1.,1.])):\n",
        "  '''\n",
        "  Computes the cross entropy cost function in the case of an artificial neural\n",
        "  network for a general classification problem, namely with an artificial neural\n",
        "  network with the number of outputs equals to the number of classes\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output, one-hot representation\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    cross entropy between the predictions of the artificial neural network\n",
        "    and the correct values\n",
        "  '''\n",
        "  y_pred = ANN(x, parameters)\n",
        "  return -jnp.mean(jnp.sum(y * class_weights * jnp.log(y_pred), axis=1))\n",
        "\n",
        "cross_entropy_general_jit = jax.jit(cross_entropy_general)"
      ],
      "metadata": {
        "id": "vnVn4awo8OT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSW(params):\n",
        "  '''\n",
        "  Computes the sum of the squared values of the weights of the artificial neural\n",
        "  network\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    sum of the squared values of the weights of the artificial neural network\n",
        "  '''\n",
        "  weights = params[::2] # extracting the weights\n",
        "  # Initializing the sum of the squared weights\n",
        "  partial_sum = 0.0\n",
        "  # Initializing the counter containing the number of parameters\n",
        "  n_weights = 0\n",
        "  for W in weights:\n",
        "    partial_sum += jnp.sum(W * W)\n",
        "    n_weights += W.shape[0] * W.shape[1]\n",
        "  return partial_sum / n_weights\n",
        "\n",
        "def cost_regularization(x, y, params, penalization, class_weights=np.array([1.,1.])):\n",
        "  '''\n",
        "  Computes the cost function applying regularization. It is used MSW for being\n",
        "  able to regularize and the cross entropy for general classification problem as\n",
        "  standard cost function\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x: ndarray\n",
        "    input of the artificial neural network\n",
        "  y: ndarray\n",
        "    correct value of the output, one-hot representation\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  penalization:\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "    \n",
        "  Returns\n",
        "  -------\n",
        "  float\n",
        "    cross entropy between the predictions of the artificial neural network\n",
        "    and the correct values with regularization term\n",
        "  '''\n",
        "  return cross_entropy_general(x, y, params, class_weights=class_weights) + penalization * MSW(params)"
      ],
      "metadata": {
        "id": "dn4qAFth-Oyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(x_train, y_train, parameters, histories, x_valid=None, y_valid=None):\n",
        "  '''\n",
        "  Updates the history of the values of the loss functions for the training set\n",
        "  and for the validation set\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    input of the artificial neural network used for the training phase\n",
        "  y_train: ndarray\n",
        "    correct value of the output of the training set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  histories: dict\n",
        "    current history of the loss functions to which a new measurement has to be\n",
        "    added\n",
        "  x_valid: ndarray, optional\n",
        "    input of the artificial neural network used for validating the model\n",
        "  y_valid: ndarray, optional\n",
        "    correct value of the output of the validation set\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  dict\n",
        "    histories of the loss functions\n",
        "  '''\n",
        "\n",
        "  if histories is None or len(histories) == 0:\n",
        "    histories = {'Xen_train': []}\n",
        "  if x_valid is not None and len(x_valid) != 0 and y_valid is not None and len(y_valid) != 0 and 'Xen_valid' not in histories.keys():\n",
        "    histories['Xen_valid'] = []\n",
        "  elif x_valid is not None and len(x_valid) != 0 and y_valid is not None and len(y_valid) != 0:\n",
        "    histories['Xen_valid'].append(cross_entropy_general_jit(x_valid, y_valid, parameters))\n",
        "  histories['Xen_train'].append(cross_entropy_general_jit(x_train, y_train, parameters))\n",
        "\n",
        "  return histories"
      ],
      "metadata": {
        "id": "vb8qbERK_beE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(histories):\n",
        "  '''\n",
        "  Plots the histories of the losses during execution\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  histories: dict\n",
        "    values of the loss functions over time to be plotted\n",
        "  '''\n",
        "  \n",
        "  fig, axs = plt.subplots(len(histories), 1, figsize=(8, 8*len(histories)))\n",
        "  axs = axs.flatten()\n",
        "  for i, key in enumerate(histories):\n",
        "    axs[i].loglog(histories[key])\n",
        "    axs[i].set_title(list(histories.keys())[i])"
      ],
      "metadata": {
        "id": "UsVH9E7OHn-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(x_train, y_train, x_valid, y_valid, parameters, num_epochs=1000, batch_size=128,\n",
        "        learning_rate_min=1e-3, learning_rate_max=1e-1, learning_rate_decay=1000,\n",
        "        penalization=1., class_weights=np.array([1.,1.])):\n",
        "  \"\"\"\n",
        "  Stochastic xgradient descent method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient\n",
        "  learning_rate_min: float, optional\n",
        "    minimum learning rate used in the training phase\n",
        "  learning_rate_max: float, optional\n",
        "    maximum learning rate used in the training phase\n",
        "  learning_rate_decay: float, optional\n",
        "    learning rate decay used in the training phase\n",
        "  penalization: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term, using penalization = 0, regularization is\n",
        "    not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "\n",
        "  num_samples = x_train.shape[0]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  # Gradient of the cost function composed by the cross entropy for general\n",
        "  # classification problems and the regularization term\n",
        "  grad_jit = jax.jit(jax.grad(cost_regularization, argnums = 2))\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], parameters, penalization, class_weights=class_weights)\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      parameters[i] -= learning_rate * grads[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "SFaH3wwKZNI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NAG(x_train, y_train, x_valid, y_valid, parameters, num_epochs=1000, batch_size=128,\n",
        "        learning_rate_min=1e-3, learning_rate_max=1e-1, learning_rate_decay=1000,\n",
        "        decay_rate=0.9, penalization=1., class_weights=np.array([1.,1.])):\n",
        "  \"\"\"\n",
        "  Implements the Nesterov accelleration method with mini-batch and learning rate\n",
        "  decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient\n",
        "  learning_rate_min: float, optional\n",
        "    minimum learning rate used in the training phase\n",
        "  learning_rate_max: float, optional\n",
        "    maximum learning rate used in the training phase\n",
        "  learning_rate_decay: float, optional\n",
        "    learning rate decay used in the training phase\n",
        "  decay_rate: float, optional\n",
        "    weight of the velocity vector at each iteration\n",
        "  penalization: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term, using penalization = 0, regularization is\n",
        "    not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "\n",
        "  num_samples = x_train.shape[0]\n",
        "  velocity = [0.0 for i in range(len(parameters))]\n",
        "  grad_args = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  # Gradient of the cost function composed by the cross entropy for general\n",
        "  # classification problems and the regularization term\n",
        "  grad_jit = jax.jit(jax.grad(cost_regularization, argnums = 2))\n",
        "  \n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    learning_rate = max(learning_rate_min, learning_rate_max * (1 - epoch/learning_rate_decay))\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      grad_args[i] = parameters[i] - decay_rate * velocity[i]\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], grad_args, penalization, class_weights=class_weights)\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      velocity[i] = decay_rate * velocity[i] + learning_rate * grads[i]\n",
        "      parameters[i] -= velocity[i]\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "7e_o_8jRYOHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSprop(x_train, y_train, x_valid, y_valid, parameters, num_epochs=1000,\n",
        "            batch_size=128, learning_rate=1e-3, decay_rate=0.8, penalization=1.,\n",
        "            class_weights=np.array([1.,1.])):\n",
        "  \"\"\"\n",
        "  Implements the RMSprop method with mini-batch and learning rate decay\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  parameters: list\n",
        "    parameters of the artificial neural network, namely weights and biases\n",
        "  num_epochs: int, optional\n",
        "    number of epochs to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient\n",
        "  learning_rate: float, optional\n",
        "    learning rate used in the training phase\n",
        "  decay_rate: float, optional\n",
        "    weight used for weighting the cumulative square grad vector at each iteration\n",
        "  penalization: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term, using penalization = 0, regularization is\n",
        "    not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    trained parameters of the artificial neural network, namely weights and\n",
        "    biases optimized for fitting the training set\n",
        "  histories: dict\n",
        "    values of the loss functions over time\n",
        "  \"\"\"\n",
        "  \n",
        "  delta = 1e-7\n",
        "  num_samples = x_train.shape[0]\n",
        "  cumulative_square_grad = [0.0 for i in range(len(parameters))]\n",
        "\n",
        "  histories = dump(x_train, y_train, parameters, None, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  # Gradient of the cost function composed by the cross entropy for general\n",
        "  # classification problems and the regularization term\n",
        "  grad_jit = jax.jit(jax.grad(cost_regularization, argnums = 2))\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    idxs = np.random.choice(num_samples, batch_size)\n",
        "    grads = grad_jit(x_train[idxs,:], y_train[idxs,:], parameters, penalization, class_weights=class_weights)\n",
        "\n",
        "    for i in range(len(parameters)):\n",
        "      cumulative_square_grad[i] = decay_rate * cumulative_square_grad[i] + (1 - decay_rate) * grads[i] * grads[i]\n",
        "      parameters[i] -= learning_rate * grads[i] / (delta + jnp.sqrt(cumulative_square_grad[i]))\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      dump(x_train, y_train, parameters, histories, x_valid=x_valid, y_valid=y_valid)\n",
        "\n",
        "  return parameters, histories"
      ],
      "metadata": {
        "id": "76DCglggZhIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_classificator_ANN(x_train, y_train, x_valid, y_valid, layers_size,\n",
        "                          opt_method='RMSprop', num_epochs=1000, batch_size=128,\n",
        "                          learning_rate=1e-3, learning_rate_min=1e-3,\n",
        "                          learning_rate_max=1e-1, learning_rate_decay=1000,\n",
        "                          history_decay=0.9, regularization_parameter=1.,\n",
        "                          class_weights=np.array([1.,1.])):\n",
        "  '''\n",
        "  Trains an artificial neural network given the labeled data.\n",
        "\n",
        "  The classificator will be created using the given sizes and activation\n",
        "  function on all the layers, except for the last one. On the last layer is\n",
        "  applied the softmax activation function.\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  x_train: ndarray\n",
        "    training set of the dataset to fit\n",
        "  y_train: ndarray\n",
        "    labels of the samples belonging to the training set\n",
        "  x_valid: ndarray\n",
        "    validation set of the dataset to fit\n",
        "  y_valid: ndarray\n",
        "    labels of the samples belonging to the validation set\n",
        "  layers_size: list \n",
        "    ordered sizes of the layers of the artificial neural network\n",
        "  opt_method: str, optional\n",
        "    chosen optimization method, the possible values are 'SGD', 'NAG' and\n",
        "    'RMSprop', stochastic gradient descent, Nesterov accelleration method and\n",
        "    RMSprop respectively\n",
        "  num_epochs: int, optional\n",
        "    number of epochs of the chosen optimization method to perform\n",
        "  batch_size: int, optional\n",
        "    size of the batches to be used for computing the gradient in the chosen\n",
        "    optimization method\n",
        "  learning_rate: float, optional\n",
        "    learning rate used in the training phase; here considered only for RMSprop\n",
        "  learning_rate_min: float, optional\n",
        "    minimum learning rate used in the training phase when using learning rate\n",
        "    decay; here considered only for SGD and NAG\n",
        "  learning_rate_max: float, optional\n",
        "    maximum learning rate used in the training phase when using learning rate\n",
        "    decay; here considered only for SGD and NAG\n",
        "  learning_rate_decay: float, optional\n",
        "    learning rate decay used in the training phase when using learning rate\n",
        "    decay; here considered only for SGD and NAG\n",
        "  history_decay: float, optional\n",
        "    weight used for weighting the history of gradients at each iteration when\n",
        "    the optimization method uses the history of gradients; here considered only\n",
        "    for NAG and RMSprop\n",
        "  regularization_parameter: float, optional\n",
        "    weight to which the MSW is multiplied and that makes possible to modify the\n",
        "    impact of the regularization term in the chosen optimization method, using\n",
        "    penalization = 0, regularization is not applied\n",
        "  class_weights: ndarray, optional\n",
        "    values used for weighting the error given by the positive class (element in \n",
        "    position 0) and by the negative class (element in position 1)\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  parameters: list\n",
        "    parameters of the trained artificial neural network\n",
        "  history: dict\n",
        "    history of the loss functions during the training phase\n",
        "  '''\n",
        "  \n",
        "  parameters = initialize_parameters(layers_size)\n",
        "  if opt_method == 'SGD':\n",
        "    ANN_params, history = SGD(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                              num_epochs=num_epochs, batch_size=batch_size, learning_rate_min=learning_rate_min,\n",
        "                              learning_rate_max=learning_rate_max, learning_rate_decay=learning_rate_decay,\n",
        "                              penalization=regularization_parameter, class_weights=class_weights)\n",
        "  elif opt_method == 'NAG':\n",
        "    ANN_params, history = NAG(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                              num_epochs=num_epochs, batch_size=batch_size, learning_rate_min=learning_rate_min,\n",
        "                              learning_rate_max=learning_rate_max, learning_rate_decay=learning_rate_decay,\n",
        "                              decay_rate=history_decay, penalization=regularization_parameter,\n",
        "                              class_weights=class_weights)\n",
        "  elif opt_method == 'RMSprop':\n",
        "    ANN_params, history = RMSprop(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                                  num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                                  decay_rate=history_decay, penalization=regularization_parameter,\n",
        "                                  class_weights=class_weights)\n",
        "  else:\n",
        "    ANN_params, history = RMSprop(x_train, y_train, x_valid, y_valid, parameters,\n",
        "                                  num_epochs=num_epochs, batch_size=batch_size, learning_rate=learning_rate,\n",
        "                                  decay_rate=history_decay, penalization=regularization_parameter,\n",
        "                                  class_weights=class_weights)\n",
        "\n",
        "  return ANN_params, history"
      ],
      "metadata": {
        "id": "XVOoSAIEH2FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_representation(y, dim):\n",
        "  y_data = np.zeros((y.shape[0], dim))\n",
        "  for i in range(dim):\n",
        "    y_data[y.reshape(y.shape[0]) == i,i] = 1\n",
        "  return y_data"
      ],
      "metadata": {
        "id": "oi794Wkmegs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and testing ANN\n",
        "\n",
        "Run the previous subsection before running the training and testing of the artificial neural network."
      ],
      "metadata": {
        "id": "h3f0bGx_GUS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the random dataset with undersampling of the\n",
        "# dataset\n",
        "\n",
        "dataset_index = 7\n",
        "# Creating the numpy array of the dataset\n",
        "dataset_np = data_7.to_numpy()\n",
        "# Splitting training set, validation set and test set\n",
        "x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "# Normalizing the training set\n",
        "x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "# Undersampling the training set\n",
        "x_train_normalized, y_train = undersample(x_train_normalized, y_train, 50.)\n",
        "\n",
        "# Normalizing the validation set\n",
        "x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "# Normalizing the test set\n",
        "x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "# Creating the onehot representation of the datasets\n",
        "dim = 2\n",
        "y_train = onehot_representation(y_train, dim)\n",
        "y_valid = onehot_representation(y_valid, dim)\n",
        "y_test = onehot_representation(y_test, dim)\n",
        "\n",
        "# Definition of the model\n",
        "layers_size = [x_train.shape[1],30,30,10,2]\n",
        "\n",
        "# Training the ANN\n",
        "ANN_params, history = fit_classificator_ANN(x_train_normalized, y_train, x_valid_normalized, y_valid,\n",
        "                                            layers_size, opt_method='RMSprop', num_epochs=2500, batch_size=256,\n",
        "                                            learning_rate=1e-3, history_decay=0.8, regularization_parameter=0.1,\n",
        "                                            class_weights=np.array([3.2,1.]))\n",
        "\n",
        "# Plotting the training history\n",
        "plot_history(history)\n",
        "\n",
        "# Getting predictions with the trained model on train set\n",
        "y_predicted = ANN(x_train_normalized, ANN_params)\n",
        "y_predicted = y_predicted >= 0.5\n",
        "# Computing metrics for the training set\n",
        "metrics_train_set = metrics(y_predicted[:,1], y_train[:,1], dataset_label='v'+str(dataset_index)+' training')\n",
        "\n",
        "# Getting predictions with the trained model on validation set\n",
        "y_predicted = ANN(x_valid_normalized, ANN_params)\n",
        "y_predicted = y_predicted >= 0.5\n",
        "# Computing metrics for the validation set\n",
        "metrics_valid_set = metrics(y_predicted[:,1], y_valid[:,1], dataset_label='v'+str(dataset_index)+' validation')\n",
        "\n",
        "# Getting predictions with the trained model on test set\n",
        "y_predicted = ANN(x_test_normalized, ANN_params) # different name for using the probabilities in the plot of the ROC curve\n",
        "y_predicted = y_predicted >= 0.5\n",
        "# Computing metrics for the test set\n",
        "metrics_test_set = metrics(y_predicted[:,1], y_test[:,1], dataset_label='v'+str(dataset_index)+' test')\n",
        "\n",
        "metrics_test_set"
      ],
      "metadata": {
        "id": "fvKIFADe7ybj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "hLu7vm7bECML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style.hide_index()"
      ],
      "metadata": {
        "id": "7NxNqC6sECVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "KVNEb352ECiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation and testing of a model for the 7 sets of selected features with\n",
        "# undersampling of the dataset\n",
        "\n",
        "datasets = [data_1, data_2, data_3, data_4, data_5, data_6, data_7]\n",
        "features_names = [v1, v2, v3, v4, v5, v6, v_random]\n",
        "metrics_train_set = None\n",
        "metrics_valid_set = None\n",
        "metrics_test_set = None\n",
        "ANN_parameters = list()\n",
        "histories = list()\n",
        "ROC_curves = dict()\n",
        "\n",
        "for i, dataset in enumerate(datasets):\n",
        "  # Creating the numpy array of the dataset\n",
        "  dataset_np = dataset.to_numpy()\n",
        "  # Splitting training set, validation set and test set\n",
        "  x_train, y_train, x_valid, y_valid, x_test, y_test = train_split(dataset_np, percentage_train=0.7, percentage_validation=0.1)\n",
        "  # Normalizing the training set\n",
        "  x_train_normalized, data_train_min, data_train_max = min_max(x_train, None, None)\n",
        "  # Undersampling the training set\n",
        "  x_train_normalized, y_train = undersample(x_train_normalized, y_train, 50.)\n",
        "\n",
        "  # Normalizing the validation set\n",
        "  x_valid_normalized, _, _ = min_max(x_valid, data_train_min, data_train_max)\n",
        "  # Normalizing the test set\n",
        "  x_test_normalized, _, _ = min_max(x_test, data_train_min, data_train_max)\n",
        "\n",
        "  # Creating the onehot representation of the datasets\n",
        "  dim = 2\n",
        "  y_train = onehot_representation(y_train, dim)\n",
        "  y_valid = onehot_representation(y_valid, dim)\n",
        "  y_test = onehot_representation(y_test, dim)\n",
        "  \n",
        "  # Definition of the model\n",
        "  layers_size = [x_train.shape[1],30,30,10,2]\n",
        "\n",
        "  # Training the ANN\n",
        "  ANN_params, history = fit_classificator_ANN(x_train_normalized, y_train, x_valid_normalized, y_valid,\n",
        "                                              layers_size, opt_method='RMSprop', num_epochs=2500, batch_size=256,\n",
        "                                              learning_rate=1e-3, history_decay=0.8, regularization_parameter=0.1,\n",
        "                                              class_weights=np.array([3.2,1.]))\n",
        "  ANN_parameters.append(ANN_params)\n",
        "  # Plotting the training history\n",
        "  histories.append(history)\n",
        "\n",
        "  # Getting predictions with the trained model on train set\n",
        "  y_predicted = ANN(x_train_normalized, ANN_params)\n",
        "  y_predicted = y_predicted >= 0.5\n",
        "  # Computing metrics for the training set\n",
        "  metrics_train_set = metrics(y_predicted[:,1], y_train[:,1], metrics_df=metrics_train_set, dataset_label='v'+str(i+1)+' training')\n",
        "  \n",
        "  # Getting predictions with the trained model on validation set\n",
        "  y_predicted = ANN(x_valid_normalized, ANN_params)\n",
        "  y_predicted = y_predicted >= 0.5\n",
        "  # Computing metrics for the validation set\n",
        "  metrics_valid_set = metrics(y_predicted[:,1], y_valid[:,1], metrics_df=metrics_valid_set, dataset_label='v'+str(i+1)+' validation')\n",
        "  \n",
        "  # Getting predictions with the trained model on test set\n",
        "  y_probabilities = ANN(x_test_normalized, ANN_params) # different name for using the probabilities in the plot of the ROC curve\n",
        "  y_predicted = y_probabilities >= 0.5\n",
        "  # Computing metrics for the test set\n",
        "  metrics_test_set = metrics(y_predicted[:,1], y_test[:,1], metrics_df=metrics_test_set, dataset_label='v'+str(i+1)+' test')\n",
        "  \n",
        "  # Compute ROC and AUC\n",
        "  fpr, tpr, _ = roc_curve(y_test[:,1], y_probabilities[:,1])\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  \n",
        "  # Plotting the ROC curve\n",
        "  plt.plot(fpr, tpr, label='AUC dataset %d = %0.2f' % (i+1, roc_auc))\n",
        "\n",
        "  # Save the ROC curve\n",
        "  ROC_curves[i] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
        "\n",
        "# Save all the ROC curves for this classifier to plot them in the end\n",
        "ROC_curves_per_classifier.append(ROC_curves)\n",
        "  \n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GJLX-ZvlLSrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the training set\n",
        "metrics_train_set.style.hide_index()"
      ],
      "metadata": {
        "id": "rbm1NF4xLSug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the metrics for the validation set\n",
        "metrics_valid_set.style.hide_index()"
      ],
      "metadata": {
        "id": "xCruJEHjLSyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the metrics\n",
        "metrics_per_classifier.append(metrics_test_set)\n",
        "\n",
        "# Displaying the metrics for the test set\n",
        "metrics_test_set.style.hide_index()"
      ],
      "metadata": {
        "id": "yS5OmvL8LS1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Metrics visualization"
      ],
      "metadata": {
        "id": "WCBCs8GeCjTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List with dataframes grouped by dataset\n",
        "metrics_per_dataset = list()\n",
        "\n",
        "for i in range(7):\n",
        "  # Create dataframe with metrics of all classifiers for each dataset\n",
        "  metric = pd.concat([metrics.iloc[[i]] for metrics in metrics_per_classifier]) \n",
        "  \n",
        "  # Drop columns 'TP', 'TN', 'FP', 'FN'\n",
        "  metric = metric.drop(metric.columns[1:5], axis=1)\n",
        "  \n",
        "  # Assign the name to each classifier\n",
        "  metric = metric.rename(columns={'Set of features': 'classifier'})\n",
        "  metric['classifier'] = ['LR', 'DT', 'RF', 'NB', 'ANN1', 'ANN2']\n",
        "\n",
        "  # Save dataframe\n",
        "  metrics_per_dataset.append(metric)\n",
        "  \n",
        "# Print all dataframes grouped by dataset\n",
        "for metrics in metrics_per_dataset:\n",
        "  # To round to two decimal places\n",
        "  # display(metrics.round(2).style.hide_index())\n",
        "  display(metrics.style.hide_index())\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "6Uif5lm-LQSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select colors of the bars\n",
        "colors = [(255/255, 204/255, 153/255), (179/255, 255/255, 153/255), (153/255, 204/255, 255/255), (255/255, 153/255, 179/255)]\n",
        "\n",
        "# Select columns to plot\n",
        "cols = ['accuracy', 'recall', 'precision', 'F1-score']\n",
        "\n",
        "# Print the metrics for all classifiers for each dataset\n",
        "for i, metric in enumerate(metrics_per_dataset):\n",
        "  # Create bar plot\n",
        "  axs = metric[cols].plot(kind='bar', color=colors)\n",
        "  axs.grid(axis='y')\n",
        "  axs.set_ylim([0, 100])\n",
        "  axs.set_xlabel('Classifier')\n",
        "  axs.set_title('Dataset ' + str(i + 1))\n",
        "  axs.set_xticklabels(metric['classifier'], rotation='horizontal')\n",
        "  # To save images \n",
        "  # name = 'Dataset_' + str(i + 1) + '.png'\n",
        "  # plt.savefig(name)\n",
        "  # files.download(name)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VrY17yFW6uFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To plot ROC curve of all classifiers for each dataset\n",
        "classifiers = ['LR', 'DT', 'RF', 'NB', 'ANN1', 'ANN2']\n",
        "\n",
        "# Select colors of the curves\n",
        "colors = ['darkviolet', 'darkorange', 'blue', 'red', 'limegreen', 'darkgreen']\n",
        "\n",
        "# Plot ROC curve of all classifiers for each dataset\n",
        "for i in range(7):\n",
        "  fig_ROC, axs_ROC = plt.subplots(1,1,figsize = (15,10))\n",
        "  for classifier,ROC_curve in enumerate(ROC_curves_per_classifier):\n",
        "    axs_ROC.plot(ROC_curve[i]['fpr'], ROC_curve[i]['tpr'], \n",
        "                    label=classifiers[classifier]+', AUC = %0.2f' % (ROC_curve[i]['auc']), color=colors[classifier])\n",
        "  axs_ROC.set_title('ROC Curve - dataset ' + str(i + 1))\n",
        "  axs_ROC.legend(loc='lower right')\n",
        "  axs_ROC.set_xlabel('False Positive Rate')\n",
        "  axs_ROC.set_ylabel('True Positive Rate')\n",
        "  # To save images \n",
        "  # name = 'ROC_' + str(i + 1) + '.png'\n",
        "  # plt.savefig(name)\n",
        "  # files.download(name)"
      ],
      "metadata": {
        "id": "ex0-uSz-DkrW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QgPAoxqUClXN",
        "KTxDeFQ3zxAb",
        "Jio8rtx05YaI",
        "mld65YMdAmQO",
        "rFOVCtun53bg",
        "c9DA4mud_MUw",
        "VVszuLMFvBul",
        "CLyQMU-KzPAZ"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}